{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhpymuZMiStNlzEA+US52R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ahtesham519/Genrative_Deep_learning_v2_2023/blob/main/Energy_Based_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6wvvkXTjkwkF"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import (\n",
        "    datasets,\n",
        "    layers,\n",
        "    models,\n",
        "    optimizers,\n",
        "    activations,\n",
        "    metrics,\n",
        "    callbacks,\n",
        "\n",
        ")\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#0.  Parameters"
      ],
      "metadata": {
        "id": "2rSLFZbbmK4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = 32\n",
        "CHANNELS = 1\n",
        "STEP_SIZE = 10\n",
        "STEPS = 60\n",
        "NOISE = 0.005\n",
        "ALPHA = 0.1\n",
        "GRADIENT_CLIP = 0.03\n",
        "BATCH_SIZE = 8192\n",
        "LEARNING_RATE = 0.0001\n",
        "EPOCHS = 60\n",
        "LOAD_MODEL = False\n"
      ],
      "metadata": {
        "id": "uCJxH1p2mJwT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the data\n",
        "(x_train, _), (x_test, _) = datasets.mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVjj4aNxmncM",
        "outputId": "b45f5dd5-ede4-448a-f9dc-6407ebc15f4c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocess the data\n",
        "\n",
        "def preprocess(imgs):\n",
        "  \"\"\"\n",
        "  Normalize and reshape the images\n",
        "  \"\"\"\n",
        "  imgs = (imgs.astype(\"float32\") - 127.5)/ 127.5\n",
        "  imgs = np.pad(imgs, ((0,0) ,( 2,2) , (2,2)) , constant_values = -1.0)\n",
        "  imgs = np.expand_dims(imgs, -1)\n",
        "  return imgs\n",
        "\n",
        "  x_train = preprocess(x_train)\n",
        "  x_test = preprocess(x_test)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "AD8oe9Eimu9V"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = tf.data.Dataset.from_tensor_slices(x_train).batch(BATCH_SIZE)\n",
        "x_test = tf.data.Dataset.from_tensor_slices(x_test).batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "UVd09MdPnRuD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Show some items of the clothing from the training set\n",
        "x_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2gj76WWngOb",
        "outputId": "152cc147-b2e7-448a-9a67-079dd20d847f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_BatchDataset element_spec=TensorSpec(shape=(None, 28, 28), dtype=tf.uint8, name=None)>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Build the EBM network"
      ],
      "metadata": {
        "id": "flMC7UdHoDic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ebm_input = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE , CHANNELS))\n",
        "x = layers.Conv2D(\n",
        "    16,kernel_size = 5, strides=2, padding= \"same\", activation= activations.swish\n",
        ")(ebm_input)\n",
        "x = layers.Conv2D(\n",
        "    32, kernel_size = 3, strides= 2 , padding = \"same\", activation = activations.swish\n",
        ")(x)\n",
        "x = layers.Conv2D(\n",
        "    64, kernel_size = 3, strides=2, padding = \"same\" , activation = activations.swish\n",
        ")(x)\n",
        "x = layers.Conv2D(\n",
        "    64, kernel_size = 3, strides = 2, padding = \"same\", activation = activations.swish\n",
        ")(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(64, activation = activations.swish)(x)\n",
        "ebm_output = layers.Dense(1)(x)\n",
        "model = models.Model(ebm_input,ebm_output)\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kw7g4KXEnq3v",
        "outputId": "f4a24937-5e4d-4c28-f806-36f54c523ad8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 32, 32, 1)]       0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 16, 16, 16)        416       \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 8, 8, 32)          4640      \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 4, 4, 64)          18496     \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 2, 2, 64)          36928     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                16448     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 76,993\n",
            "Trainable params: 76,993\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if LOAD_MODEL:\n",
        "  model.load_weights(\"./models/model.h5\")"
      ],
      "metadata": {
        "id": "wf7OYw1bpnD4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Set up a Langevin sampler function"
      ],
      "metadata": {
        "id": "mAMextZKp9pn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to generate samples using Langevin Dynamics\n",
        "def generate_samples(\n",
        "    model, inps_imgs, steps, step_size, noise, return_img_per_step = False\n",
        "):\n",
        "    imgs_per_step = []\n",
        "    for _ in range(steps):\n",
        "      inp_imgs += tf.random.normal(inp_imgs.shape, mean = 0 , stddev = noise)\n",
        "      inp_imgs = tf.clip_by_value(inp_imgs, -1.0 , 1.0)\n",
        "      with tf.GradientTape() as tape:\n",
        "        tape.watch(inp_imgs)\n",
        "        out_score = model(inp_imgs)\n",
        "      grads = tape.gradient(out_score, inp_imgs)\n",
        "      grads = tf.clip_by_value(grads, -GRADIENT_CLIP , GRADIENT_CLIP)\n",
        "      inp_imgs += step_size * grads\n",
        "      inp_imgs = tf.clip_by_value(inp_imgs, -1.0 , 1.0)\n",
        "      if return_img_per_step:\n",
        "        imgs_per_step.append(inp_imgs)\n",
        "    if return_img_per_step:\n",
        "      return tf.stack(imgs_per_step, axis = 0)\n",
        "    else:\n",
        "      return inp_imgs"
      ],
      "metadata": {
        "id": "oo_7dEAsp9DT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7VyKlmSirhnI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}