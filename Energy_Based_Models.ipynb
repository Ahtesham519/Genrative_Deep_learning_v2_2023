{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2jERHWANXw3IhYwe98RcP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ahtesham519/Genrative_Deep_learning_v2_2023/blob/main/Energy_Based_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6wvvkXTjkwkF"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import (\n",
        "    datasets,\n",
        "    layers,\n",
        "    models,\n",
        "    optimizers,\n",
        "    activations,\n",
        "    metrics,\n",
        "    callbacks,\n",
        "\n",
        ")\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#0.  Parameters"
      ],
      "metadata": {
        "id": "2rSLFZbbmK4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = 32\n",
        "CHANNELS = 1\n",
        "STEP_SIZE = 10\n",
        "STEPS = 60\n",
        "NOISE = 0.005\n",
        "ALPHA = 0.1\n",
        "GRADIENT_CLIP = 0.03\n",
        "BATCH_SIZE = 128\n",
        "BUFFER_SIZE = 8192\n",
        "LEARNING_RATE = 0.0001\n",
        "EPOCHS = 60\n",
        "LOAD_MODEL = False\n"
      ],
      "metadata": {
        "id": "uCJxH1p2mJwT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the data\n",
        "(x_train, _), (x_test, _) = datasets.mnist.load_data()"
      ],
      "metadata": {
        "id": "vVjj4aNxmncM"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocess the data\n",
        "\n",
        "def preprocess(imgs):\n",
        "  \"\"\"\n",
        "  Normalize and reshape the images\n",
        "  \"\"\"\n",
        "  imgs = (imgs.astype(\"float32\") - 127.5)/ 127.5\n",
        "  imgs = np.pad(imgs, ((0,0) ,( 2,2) , (2,2)) , constant_values = -1.0)\n",
        "  imgs = np.expand_dims(imgs, -1)\n",
        "  return imgs\n",
        "\n",
        "  x_train = preprocess(x_train)\n",
        "  x_test = preprocess(x_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "AD8oe9Eimu9V"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = tf.data.Dataset.from_tensor_slices(x_train).batch(BATCH_SIZE)\n",
        "x_test = tf.data.Dataset.from_tensor_slices(x_test).batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "UVd09MdPnRuD"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Show some items of the clothing from the training set\n",
        "x_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2gj76WWngOb",
        "outputId": "a6ac803a-58c2-48dd-ad22-68f18b843da2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_BatchDataset element_spec=TensorSpec(shape=(None, 28, 28), dtype=tf.uint8, name=None)>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Build the EBM network"
      ],
      "metadata": {
        "id": "flMC7UdHoDic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ebm_input = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE , CHANNELS))\n",
        "x = layers.Conv2D(\n",
        "    16,kernel_size = 5, strides=2, padding= \"same\", activation= activations.swish\n",
        ")(ebm_input)\n",
        "x = layers.Conv2D(\n",
        "    32, kernel_size = 3, strides= 2 , padding = \"same\", activation = activations.swish\n",
        ")(x)\n",
        "x = layers.Conv2D(\n",
        "    64, kernel_size = 3, strides=2, padding = \"same\" , activation = activations.swish\n",
        ")(x)\n",
        "x = layers.Conv2D(\n",
        "    64, kernel_size = 3, strides = 2, padding = \"same\", activation = activations.swish\n",
        ")(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(64, activation = activations.swish)(x)\n",
        "ebm_output = layers.Dense(1)(x)\n",
        "model = models.Model(ebm_input,ebm_output)\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kw7g4KXEnq3v",
        "outputId": "7136490d-3a14-4ad5-b830-fe24ac655993"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 32, 32, 1)]       0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 16, 16, 16)        416       \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 8, 8, 32)          4640      \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 4, 4, 64)          18496     \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 2, 2, 64)          36928     \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 64)                16448     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 76,993\n",
            "Trainable params: 76,993\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if LOAD_MODEL:\n",
        "  model.load_weights(\"./models/model.h5\")"
      ],
      "metadata": {
        "id": "wf7OYw1bpnD4"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Set up a Langevin sampler function"
      ],
      "metadata": {
        "id": "mAMextZKp9pn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to generate samples using Langevin Dynamics\n",
        "def generate_samples(\n",
        "    model, inps_imgs, steps, step_size, noise, return_img_per_step = False\n",
        "):\n",
        "    imgs_per_step = []\n",
        "    for _ in range(steps):\n",
        "      inp_imgs += tf.random.normal(inp_imgs.shape, mean = 0 , stddev = noise)\n",
        "      inp_imgs = tf.clip_by_value(inp_imgs, -1.0 , 1.0)\n",
        "      with tf.GradientTape() as tape:\n",
        "        tape.watch(inp_imgs)\n",
        "        out_score = model(inp_imgs)\n",
        "      grads = tape.gradient(out_score, inp_imgs)\n",
        "      grads = tf.clip_by_value(grads, -GRADIENT_CLIP , GRADIENT_CLIP)\n",
        "      inp_imgs += step_size * grads\n",
        "      inp_imgs = tf.clip_by_value(inp_imgs, -1.0 , 1.0)\n",
        "      if return_img_per_step:\n",
        "        imgs_per_step.append(inp_imgs)\n",
        "    if return_img_per_step:\n",
        "      return tf.stack(imgs_per_step, axis = 0)\n",
        "    else:\n",
        "      return inp_imgs"
      ],
      "metadata": {
        "id": "oo_7dEAsp9DT"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Set up a buffer to store examples"
      ],
      "metadata": {
        "id": "bJCq4_28Rx9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Buffer:\n",
        "  def __init__(self, model):\n",
        "    super().__init__()\n",
        "    self.model = model\n",
        "    self.examples = [\n",
        "        tf.random.uniform(shape =(1, IMAGE_SIZE , IMAGE_SIZE , CHANNELS)) * 2\n",
        "        -1\n",
        "        for _ in range(BATCH_SIZE)\n",
        "    ]\n",
        "\n",
        "  def sample_new_exmps(self, steps,step_size , noise):\n",
        "    n_new = np.random.bionomial(BATCH_SIZE , 0.05)\n",
        "    rand_imgs = (\n",
        "        tf.random.uniform((n_new , IMAGE_SIZE, IMAGE_SIZE, CHANNELS))* 2-1\n",
        "    )\n",
        "    old_imgs = tf.concat(\n",
        "          random.choices(self.examples, k = BATCH_SIZE - n_new), axis = 0\n",
        "    )\n",
        "    inp_imgs = tf.concat([rand_imgs, old_imgs] , axis = 0)\n",
        "    inp_imgs = generate_samples(\n",
        "        self.model, inp_imgs, steps = steps , step_size = step_size, noise = noise\n",
        "    )\n",
        "    self.examples = tf.split(inp_imgs, BATCH_SIZE, axis = 0) + self.examples\n",
        "    self.examples = self.examples[:BUFFER_SIZE]\n",
        "    return inp_imgs"
      ],
      "metadata": {
        "id": "7VyKlmSirhnI"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EBM(models.Model):\n",
        "  def __init__ (self):\n",
        "    super(EBM, self).__init__()\n",
        "    self.model = model\n",
        "    self.buffer = Buffer(self.model)\n",
        "    self.alpha = ALPHA\n",
        "    self.loss_metric = metrics.Mean(name = \"loss\")\n",
        "    self.reg_loss_metric = metrics.Mean(name= \"reg\")\n",
        "    self.cdiv_loss_metric = metrics.Mean(name= 'cdiv')\n",
        "    self.real_out_metric = metrics.Mean(name = \"real\")\n",
        "    self.fake_out_metric = metrics.Mean(name = \"fake\")\n",
        "\n",
        "  @property\n",
        "  def metrics(self):\n",
        "    return [\n",
        "        self.loss_metric,\n",
        "        self.reg_loss_metric,\n",
        "        self.cdiv_loss_metric,\n",
        "        self.real_out_metric,\n",
        "        self.fake_out_metric,\n",
        "\n",
        "    ]\n",
        "\n",
        "  def train_step(self, real_imgs):\n",
        "    real_imgs += tf.random.normal(\n",
        "        shape= tf.shape(real_imgs), mean = 0, stddev = NOISE\n",
        "    )\n",
        "    real_imgs = tf.clip_by_value(real_imgs, -1.0, 1.0)\n",
        "    fake_imgs = self.buffer.sample_new_exmps(\n",
        "        steps = STEPS, step_size = STEP_SIZE , noise = NOISE\n",
        "    )\n",
        "    inp_imgs = tf.concat([real_imgs, fake_imgs], axis = 0)\n",
        "    with tf.GradientTape() as training_tape:\n",
        "      real_out , fake_out = tf.split(self.model(inp_imgs) , 2, axis = 0)\n",
        "      cdiv_loss = tf.reduce_mean(fake_out, axis = 0) - tf.reduce_mean(\n",
        "          real_out, axis = 0\n",
        "      )\n",
        "      reg_loss = self.alpha * tf.reduce_mean(\n",
        "          real_out **2 + fake_out **2 , axis = 0\n",
        "      )\n",
        "      loss = cdiv_loss +reg_loss\n",
        "      grads = training_tape.gradient(loss, self.model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(\n",
        "          zip(grads, self.model.trainable_variables)\n",
        "      )\n",
        "      self.loss_metric.update_state(loss)\n",
        "      self.reg_loss_metric.update_state(reg_loss)\n",
        "      self.cdiv_loss_metric.update_state(cdiv_loss)\n",
        "      self.real_out_metric.update_state(tf.reduce_mean(real_out, axis =0))\n",
        "      self.fake_out_metric.update_state(tf.reduce_mean(fake_out, axis = 0))\n",
        "      return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def test_step(self, real_imgs):\n",
        "      batch_size = real_imgs.shape[0]\n",
        "      fake_imgs = (\n",
        "          tf.random.uniform((batch_size, IMAGE_SIZE , IMAGE_SIZE , CHANNELS))\n",
        "          * 2\n",
        "          - 1\n",
        "      )\n",
        "      inp_imgs = tf.concat([real_imgs, fake_imgs] , axis = 0)\n",
        "      real_out, fake_out = tf.split(self.model(inp_imgs) , 2, axis = 0)\n",
        "      cdiv = tf.reduce_mean(fake_out, axis = 0) - tf.reduce_mean(\n",
        "          real_out , axis = 0\n",
        "      )\n",
        "      self.cdiv_loss_metric.update_state(cdiv)\n",
        "      self.real_out_metric.update_state(tf.reduce_mean(real_out, axis = 0))\n",
        "      self.fake_out_metric.update_state(tf.reduce_mean(fake_out, axis = 0))\n",
        "      return {m.name : m.result() for m in self.metrics[2:]}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mAyzEESnTYWH"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ebm = EBM()"
      ],
      "metadata": {
        "id": "UFvDYBuoTdzW"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Train the EBM network"
      ],
      "metadata": {
        "id": "MXt6Jq0hYDUn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "73zQphlVYCEt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}