{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOB7be32DNA0NW+sYZDHqHW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ahtesham519/Genrative_Deep_learning_v2_2023/blob/main/GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GPT"
      ],
      "metadata": {
        "id": "j8JvX5ofa1Ui"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0p7hLTcaskn"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "import string\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models , losses , callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#0. Parameters"
      ],
      "metadata": {
        "id": "Uy46hwL-bRBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = 10000\n",
        "MAX_LEN = 80\n",
        "EMBEDDING_DIM = 256\n",
        "KEY_DIM = 256\n",
        "N_HEAD = 2\n",
        "FEED_FORWARD_DIM = 256\n",
        "VALIDATION_SPLIT = 0.2\n",
        "SEED = 42\n",
        "LOAD_MODEL = False\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 5\n"
      ],
      "metadata": {
        "id": "q7Q0ucIjbP-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Load the Data"
      ],
      "metadata": {
        "id": "tDcSUoiebx3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!echo '{\"username\":\"ahtesham\",\"key\":\"cee6ab1f831fe575d0f17c5bf9ea218b\"}' > ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "XM_zilYldVqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d boh010/winemagdata130kv2json -p /content\n"
      ],
      "metadata": {
        "id": "vsxJZp2CdZ-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/winemagdata130kv2json.zip -d /content/winemagdata\n"
      ],
      "metadata": {
        "id": "CCLW16cDd04G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the full dataset\n",
        "with open(\"/content/winemagdata/winemag-data-130k-v2.json\") as json_data:\n",
        "  wine_data = json.load(json_data)"
      ],
      "metadata": {
        "id": "iWAVsfy2bw6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wine_data[10]"
      ],
      "metadata": {
        "id": "b2UmAZc5cDFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Filter the dataset\n",
        "filtered_data = [\n",
        "    \"wine review :\" + x[\"country\"] + \" : \" + x[\"variety\"] + \" :\" + x[\"description\"] for x in wine_data if x[\"country\"] is not None and x[\"province\"] is not None and x[\"variety\"] is not None and x[\"description\"] is not None\n",
        "]"
      ],
      "metadata": {
        "id": "s98btUdaeNMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#count the recipes\n",
        "n_wines = len(filtered_data)\n",
        "print(f\"{n_wines} recipes loaded\")"
      ],
      "metadata": {
        "id": "KcUcojIGfFNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = filtered_data[25]\n",
        "print(example)"
      ],
      "metadata": {
        "id": "3H6HneGdfPCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Tokenize the data"
      ],
      "metadata": {
        "id": "VLsEc_lNfhH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Pad the punctuation to treat them as seperate 'word'\n",
        "\n",
        "def pad_punctuation(s):\n",
        "  s = re.sub(f\"([{string.punctuation}, '\\n'])\", r\" \\1 \", s)\n",
        "  s = re.sub(\" + \", \" \", s)\n",
        "  return s\n",
        "\n",
        "\n",
        "text_data = [pad_punctuation(x) for x in filtered_data]"
      ],
      "metadata": {
        "id": "3T6d2wirff80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Display an example of a recipe\n",
        "example_data = text_data[40]\n",
        "example_data"
      ],
      "metadata": {
        "id": "tqxoNwTKgC9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert to a Tensorflow Dataset\n",
        "text_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices(text_data)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .shuffle(1000)\n",
        ")"
      ],
      "metadata": {
        "id": "hoHKVOkvgaiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a vectorisation layer\n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    standardize = \"lower\",\n",
        "    max_tokens = VOCAB_SIZE,\n",
        "    output_mode = \"int\",\n",
        "    output_sequence_length = MAX_LEN + 1,\n",
        ")"
      ],
      "metadata": {
        "id": "9LV56pJkhGwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adapt the layer to the training set\n",
        "vectorize_layer.adapt(text_ds)\n",
        "vocab = vectorize_layer.get_vocabulary()\n"
      ],
      "metadata": {
        "id": "dJhIubLIhbxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Display some token:word mappings\n",
        "for i, word in enumerate(vocab[:10]):\n",
        "  print(f\"{i}:{word}\")\n"
      ],
      "metadata": {
        "id": "4I76DZZnhv-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Display the same example converted to ints\n",
        "example_tokenised = vectorize_layer(example_data)\n",
        "print(example_tokenised.numpy())"
      ],
      "metadata": {
        "id": "08P2ODd9h7YD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Create the Training Set"
      ],
      "metadata": {
        "id": "EuLRPWh4hC5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the training set of recipes and the same text shifted by one word\n",
        "\n",
        "def prepare_inputs(text):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  tokenized_sentences = vectorize_layer(text)\n",
        "  x = tokenized_sentences[: , : -1]\n",
        "  y = tokenized_sentences[: , 1 :]\n",
        "  return x, y\n",
        "\n",
        "train_ds = text_ds.map(prepare_inputs)\n"
      ],
      "metadata": {
        "id": "_HC04UIpiJBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_input_output = train_ds.take(1).get_single_element()\n"
      ],
      "metadata": {
        "id": "772GOBWphrs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Example Input\n",
        "example_input_output[0][0]"
      ],
      "metadata": {
        "id": "09pSecBph4ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#example Output(shifted by one token)\n",
        "example_input_output[1][0]"
      ],
      "metadata": {
        "id": "f9lTl9CKh968"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Create the causal attention mask function"
      ],
      "metadata": {
        "id": "lJg5oJFciGlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def causal_attention_mask(batch_size, n_dest , n_src, dtype):\n",
        "  i = tf.range(n_dest)[:, None]\n",
        "  j = tf.range(n_src)\n",
        "  m = i >= j - n_src +n_dest\n",
        "  mask = tf.cast(m, dtype)\n",
        "  mask = tf.reshape(mask, [1, n_dest, n_src])\n",
        "  mult = tf.concat(\n",
        "      [tf.expand_dims(batch_size, -1) , tf.constant([1,1], dtype = tf.int32)] , 0\n",
        "  )\n",
        "  return tf.tile(mask , mult)\n",
        "\n",
        "np.transpose(causal_attention_mask(1, 10, 10, dtype = tf.int32)[0])\n"
      ],
      "metadata": {
        "id": "DcYz509ZiFXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Create a Transformer Block Layer"
      ],
      "metadata": {
        "id": "ESgXiGivjCzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.terminal.embed import embed\n",
        "class TransformerBlock(layers.Layer):\n",
        "  def __init__(self, num_heads , key_dim , embed_dim , ff_dim , dropout_rate = 0.1):\n",
        "    super(TransformerBlock, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.key_dim = key_dim\n",
        "    self.embed_dim = embed_dim\n",
        "    self.ff_dim = ff_dim\n",
        "    self.dropout_rate = dropout_rate\n",
        "    self.attn = layers.MultiHeadAttention(\n",
        "        num_heads, key_dim , output_shape = embed_dim\n",
        "    )\n",
        "    self.dropout_1 = layers.Dropout(self.dropout_rate)\n",
        "    self.ln_1 = layers.LayerNormalization(epsilon = 1e-6)\n",
        "    self.ffn_1 = layers.Dense(self.ff_dim , activation = \"relu\")\n",
        "    self.ffn_2 = layers.Dense(self.embed_dim)\n",
        "    self.dropout_2 = layers.Dropout(self.dropout_rate)\n",
        "    self.ln_2 = layers.LayerNormalization(epsilon = 1e-6)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    input_shape = tf.shape(inputs)\n",
        "    batch_size = input_shape[0]\n",
        "    seq_len = input_shape[1]\n",
        "    causal_mask = causal_attention_mask(\n",
        "        batch_size , seq_len , seq_len , tf.bool\n",
        "    )\n",
        "    attention_output, attention_scores = self.attn(\n",
        "        inputs,\n",
        "        inputs,\n",
        "        attention_mask = causal_mask,\n",
        "        return_attention_scores = True,\n",
        "    )\n",
        "    attention_output = self.dropout_1(attention_output)\n",
        "    out1 = self.ln_1(inputs +attention_output)\n",
        "    ffn_1 = self.ffn_1(out1)\n",
        "    ffn_2 = self.ffn_2(ffn_1)\n",
        "    ffn_output = self.dropout_2(ffn_2)\n",
        "    return (self.ln_2(out1 + ffn_output) , attention_scores)\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update(\n",
        "        {\n",
        "            \"key_dim\" : self.key_dim,\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\" : self.num_heads,\n",
        "            \"ff_dim\": self.ff_dim,\n",
        "            \"dropout_rate\": self.dropout_rate,\n",
        "        }\n",
        "    )\n",
        "    return config\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pONZ2KSxjBq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. Create the token and position"
      ],
      "metadata": {
        "id": "tLyMQ-z_t2Yh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "  def __init__(self, max_len , vocab_size , embed_dim):\n",
        "    super(TokenAndPositionEmbedding , self).__init__()\n",
        "    self.max_len = max_len\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embed_dim = embed_dim\n",
        "    self.token_emb = layers.Embedding(\n",
        "        input_dim = vocab_size , output_dim = embed_dim\n",
        "    )\n",
        "    self.pos_emb = layers.Embedding(input_dim = max_len , output_dim = embed_dim)\n",
        "\n",
        "  def call(self , x):\n",
        "    maxlen = tf.shape(x)[-1]\n",
        "    positions = tf.range(start = 0 , limit = maxlen , delta = 1)\n",
        "    positions = self.pos_emb(positions)\n",
        "    x = self.token_emb(x)\n",
        "    return x + positions\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update(\n",
        "        {\n",
        "            \"max_len\": self.max_len,\n",
        "            \"vocab_size\": self.vocab_size,\n",
        "            \"embed_dim\" : self.embed_dim,\n",
        "        }\n",
        "    )\n",
        "    return config"
      ],
      "metadata": {
        "id": "EmrVyWHRt18J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. Build the Transformer Model"
      ],
      "metadata": {
        "id": "I6GlQjTivQMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = layers.Input(shape = (None, ), dtype= tf.int32)\n",
        "x = TokenAndPositionEmbedding(MAX_LEN , VOCAB_SIZE , EMBEDDING_DIM)(inputs)\n",
        "x, attention_scores = TransformerBlock(\n",
        "    N_HEAD, KEY_DIM, EMBEDDING_DIM , FEED_FORWARD_DIM\n",
        ")(x)\n",
        "outputs = layers.Dense(VOCAB_SIZE, activation = \"softmax\")(x)\n",
        "gpt = models.Model(inputs = inputs, outputs = [outputs, attention_scores])\n",
        "gpt.compile(\"adam\" , loss = [losses.SparseCategoricalCrossentropy() , None])\n"
      ],
      "metadata": {
        "id": "ZQ0kZAZivPF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt.summary()"
      ],
      "metadata": {
        "id": "gKTPUPxPwcQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if LOAD_MODEL:\n",
        "  #model.load_weight('./models/model)\n",
        "  gpt = models.load_model(\"./models/gpt\", compile = True)"
      ],
      "metadata": {
        "id": "9u_nKB8zwfOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9. Train the Transformer"
      ],
      "metadata": {
        "id": "orSNNDlwy9Yv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a TextGenrator checkpoint\n",
        "class TextGenerator(callbacks.Callback):\n",
        "  def __init__(self, index_to_word , top_k = 10):\n",
        "    self.index_to_word = index_to_word\n",
        "    self.word_to_word = {\n",
        "        word: index for index, word in enumerate(index_to_word)\n",
        "    }\n",
        "\n",
        "  def sample_from(self, probs , temperature):\n",
        "    probs = probs ** (1 / temperature)\n",
        "    probs = probs / np.sum(probs)\n",
        "    return np.random.choice(len(probs) , p = probs), probs\n",
        "\n",
        "  def generate(self, start_prompt , max_tokens,  temperature):\n",
        "    start_tokens = [\n",
        "        self.word_to_index.get(x, 1) for x in start_prompt.split()\n",
        "    ]\n",
        "    sample_token = None\n",
        "    info = []\n",
        "    while len(start_tokens) < max_tokens and sample_token !=0:\n",
        "      x = np.array([start_tokens])\n",
        "      y, att = self.model.predict(x, verbose = 0)\n",
        "      sample_token, probs = self.sample_from(y[0][-1], temperature)\n",
        "      info.append(\n",
        "          {\n",
        "              \"prompt\": start_prompt,\n",
        "              \"word_probs\" :probs,\n",
        "              \"atts\" :att[0, :, -1 , :],\n",
        "          }\n",
        "      )\n",
        "      start_tokens.append(sample_token)\n",
        "      start_prompt = start_prompt + \"\" + self.index_to_word[sample_token]\n",
        "    print(f\"\\ngenerated text :\\n{start_prompt}\")\n",
        "    return info\n",
        "\n",
        "  def on_epoch_end(self, epoch , logs = None):\n",
        "    self.generate(\"wine Review\", max_tokens = 80, temperature = 1.0)\n"
      ],
      "metadata": {
        "id": "lsA4TmrZwqHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a model save the checkpoint\n",
        "model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
        "    filepath = \"./checkpoint.ckpt\",\n",
        "    save_weights_only = True,\n",
        "    save_freq = \"epoch\",\n",
        "    verbose = 0 ,\n",
        ")\n",
        "\n",
        "tensorboard_callback = callbacks.TensorBoard(log_dir = \"./logs\")\n",
        "\n",
        "#Tokenize starting prompt\n",
        "text_generator = TextGenerator(vocab)"
      ],
      "metadata": {
        "id": "J4TRrPz31Iyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt.fit(\n",
        "    train_ds,\n",
        "    epochs = EPOCHS,\n",
        "    callbacks = [model_checkpoint_callback , tensorboard_callback, text_generator],\n",
        ")"
      ],
      "metadata": {
        "id": "7lW7bAIctWbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save the final model\n",
        "gpt.save(\"./models/gpt\")\n"
      ],
      "metadata": {
        "id": "C9AIf8oxthPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Generate text using the transformer"
      ],
      "metadata": {
        "id": "Si0gPJk8tqHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_probs(info, vocab , top_k = 5):\n",
        "  for i in info:\n",
        "    highlighted_text = []\n",
        "    for word, att_score in zip(\n",
        "        i[\"prompt\"].split(), np.mean(i[\"atts\"], axis = 0)\n",
        "    ):\n",
        "      highlighted_text.append(\n",
        "         '<span style=\"background-color:rgba(135, 206 , 250,'\n",
        "          + str(att_score / max(np.mean(i[\"atts\"], axis = 0)))\n",
        "          + ');\">'\n",
        "          + word\n",
        "          + \"</span>\"\n",
        "         )\n",
        "      highlighted_text = \" \".join(highlighted_text)\n",
        "      display(HTML(highlighted_text))\n",
        "\n",
        "      word_probs = i[\"word_probs\"]\n",
        "      p_sorted = np.sort(word_probs)[:: -1][:top_k]\n",
        "      i_sorted = np.argsort(word_probs)[:: -1][:top_k]\n",
        "      for p, i in zip(p_sorted , i_sorted):\n",
        "        print(f\"{vocab[i]}:     \\t{np.round(100*p, 2)}%\")\n",
        "      print(\"---------\\n\")\n",
        ""
      ],
      "metadata": {
        "id": "sh9CDIbXtnJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    \"wine review : us\" , max_tokens = 80, temperature = 1.0\n",
        ")"
      ],
      "metadata": {
        "id": "8C8CEwfR4i28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generate(\n",
        "    \"wine review : italy\" , max_tokens = 80 , temperature = 0.5\n",
        ")"
      ],
      "metadata": {
        "id": "1cVpwmZaURqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    \"wine review : germany\" , max_tokens = 80 , temperature = 0.5\n",
        ")\n",
        "print_probs(info, vocab)"
      ],
      "metadata": {
        "id": "lEvc-_NbUfcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w2pu2W7cUso7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}