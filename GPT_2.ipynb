{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhF5OdxgUlgCQWsk+fNwUJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ahtesham519/Genrative_Deep_learning_v2_2023/blob/main/GPT_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building my own gpt model on the wine dataset"
      ],
      "metadata": {
        "id": "y98j4B4cJ-xT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVSJsjfLJ59-"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "import string\n",
        "from IPython.display import display , HTML\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models , losses , callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#0. Parameters"
      ],
      "metadata": {
        "id": "yLq18RbLKaao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = 10000\n",
        "MAX_LEN = 80\n",
        "EMBEDDING_DIM = 256\n",
        "KEY_DIM = 256\n",
        "N_HEADS = 2\n",
        "FEED_FORWARD_DIM = 256\n",
        "VALIDATION_SPLIT = 0.2\n",
        "SEED = 42\n",
        "LOAD_MODEL = False\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 5\n"
      ],
      "metadata": {
        "id": "Xabnwb-PKZdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Load the Data"
      ],
      "metadata": {
        "id": "i0qEj3QTK3fM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the full dataset\n",
        "with open(\"./app/data/wine-reviews/winemag-data-130k-v2.json\") as json_data:\n",
        "  wine_data = json.load(json_data)"
      ],
      "metadata": {
        "id": "eGzKbHzzK1sq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wine_data[10]"
      ],
      "metadata": {
        "id": "Y6fMSi-kLHqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#filter the dataset\n",
        "filtered_data = [\n",
        "    \"wine review :\"\n",
        "    + x[\"country\"]\n",
        "    + \": \"\n",
        "    + x[\"province\"]\n",
        "    + \" : \"\n",
        "    + x[\"variety\"]\n",
        "    +\":\"\n",
        "    +x[\"description\"]\n",
        "    for x in wine_data\n",
        "    if x[\"country\"] is not None\n",
        "    and x[\"province\"] is not None\n",
        "    and x[\"variety\"] is not None\n",
        "    and x[\"description\"] is not None\n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "HyeOUawxLKoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Count the recipes\n",
        "n_wines = len(filtered_data)\n",
        "print(f\"{n_wines} recipes loaded\")"
      ],
      "metadata": {
        "id": "Sd357dqPLy_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = filtered_data[25]\n",
        "print(example)"
      ],
      "metadata": {
        "id": "QR6muAMoL-C9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Tokenize the data"
      ],
      "metadata": {
        "id": "AEXB0ZoNMCmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Pad the punctuation , to treat them as seperate 'words'\n",
        "def pad_punctuation(s):\n",
        "  s = re.sub(f\"({string.punctuation}, '\\n')\", r\"\\1\", s)\n",
        "  s = re.sub(\" + \", \" \" , s)\n",
        "  return s\n",
        "\n",
        "text_data = [pad_punctuation(x) for x in filtered_data]"
      ],
      "metadata": {
        "id": "bCw6JaWkMBsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Display an example of a recipe\n",
        "example_data = text_data[25]\n",
        "example_data"
      ],
      "metadata": {
        "id": "qvfUk0qBMjcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert to a tensorflow dataset\n",
        "text_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices(text_data)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .shuffle(1000)\n",
        ")"
      ],
      "metadata": {
        "id": "P2pHQp4oMqCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a vectorization layer\n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    standardize = \"lower\",\n",
        "    max_tokens = VOCAB_SIZE,\n",
        "    output_mode = \"int\",\n",
        "    output_sequence_length = MAX_LEN + 1,\n",
        ")"
      ],
      "metadata": {
        "id": "3jWwMJRcM22m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Adapt the layer to the training set\n",
        "vectorize_layer.adapt(text_ds)\n",
        "vocab = vectorize_layer.get_vocabulary()"
      ],
      "metadata": {
        "id": "npQUluxrNLbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Display some token:word mappings\n",
        "for i , word in enumerate(vocab[:10]):\n",
        "  print(f\"{i} : {word}\")"
      ],
      "metadata": {
        "id": "U-1eTIOJNVjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Display the same example converted to ints\n",
        "example_tokenised = vectorize_layer(example_data)\n",
        "print(example_tokenized.numpy())\n"
      ],
      "metadata": {
        "id": "8zz8WnHRNfi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Create the training set"
      ],
      "metadata": {
        "id": "7-ZwJPT1NsvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the training set of recipes and the same text shifted by one word\n",
        "\n",
        "def prepare_inputs(text):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  tokenized_sentences = vectorize_layer(text)\n",
        "  x = tokenized_sentences[:, :-1]\n",
        "  y = tokenized_sentences[:, 1:]\n",
        "  return x, y\n",
        "\n",
        "train_ds = text_ds.map(prepare_inputs)"
      ],
      "metadata": {
        "id": "7hOlKIF6Nr1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_input_output = train_ds.take(1).get_single_element()"
      ],
      "metadata": {
        "id": "jKP9UlLzOsSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#example Input\n",
        "example_input_output[0][0]"
      ],
      "metadata": {
        "id": "H-OdJBTOOyfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Example output (shifted by one token)\n",
        "example_input_output[1][0]"
      ],
      "metadata": {
        "id": "_vKd7zDTO3U8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Create the casual attention mask function"
      ],
      "metadata": {
        "id": "A0DOPFC7O-QY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def casual_attention_mask(batch_size, n_dest, n_src, dtype):\n",
        "  i = tf.range(n_dest)[:, None]\n",
        "  j = tf.range(n_src)\n",
        "  m = i >= j - n_src + n_dest\n",
        "  mask = tf.cast(m, dtype)\n",
        "  mask = tf.reshape(mask, [1, n_dest , n_src])\n",
        "  mult = tf.concat(\n",
        "      [tf.expand_dims(batch_size , -1) , tf.constant([1,1] , dtype = tf.int32)], 0\n",
        "  )\n",
        "  return tf.tile(mask , mult)\n",
        "\n",
        "np.transpose(casual_attention_mask(1, 10, 10, dtype = tf.int32)[0])"
      ],
      "metadata": {
        "id": "vK6RChz6O9Ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Create a Transformer Block layer"
      ],
      "metadata": {
        "id": "LTYX2O1VPwpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "  def __init__(self, num_heads, key_dim , embed_dim , ff_dim , dropout_rate = 0.1):\n",
        "    super(TransformerBlock, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.key_dim = key_dim\n",
        "    self.embed_dim = embed_dim\n",
        "    self.ff_dim = ff_dim\n",
        "    self.dropout_rate = dropout_rate\n",
        "    self.attn = layers.MultiHeadAttention(\n",
        "        num_heads , key_dim , output_shape = embed_dim\n",
        "    )\n",
        "    self.dropout_1 = layers.Dropout(self.dropout_rate)\n",
        "    self.ln_1 = layers.LayerNormalization(epsilon = 1e-6)\n",
        "    self.ffn_1 = layers.Dense(self.ff_dim , activation= \"relu\")\n",
        "    self.ffn_2 = layers.Dense(self.embed_dim)\n",
        "    self.dropout_2 = layers.Dropout(self.dropout_rate)\n",
        "    self.ln_2 = layers.LayerNormalization(epsilon = 1e-6)\n",
        "\n",
        "  def call(self, inputs ):\n",
        "    input_shape = tf.shape(inputs)\n",
        "    batch_size = input_shape[0]\n",
        "    seq_len = input_shape[1]\n",
        "    casual_mask = casual_attention_mask(\n",
        "        batch_size , seq_len , seq_len , tf.bool\n",
        "    )\n",
        "    attention_output , attention_scores = self.attn(\n",
        "        inputs,\n",
        "        inputs,\n",
        "        attention_mask = casual_mask ,\n",
        "        return_attention_scores = True ,\n",
        "    )\n",
        "    attention_output = self.dropout_1(attention_output)\n",
        "    out1 = self.ln_1(inputs + attention_output)\n",
        "    ffn_1 = self.ffn_1(out1)\n",
        "    ffn_2 = self.ffn_2(ffn_1)\n",
        "    ffn_output = self.dropout_2(ffn_2)\n",
        "    return ( self.ln_2(out1 + ffn_output) , attention_scores)\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update(\n",
        "        {\n",
        "        \"key_dim\" :self.key_dim,\n",
        "        \"embed_dim\" : self.embed_dim,\n",
        "        \"num_heads\" :self.num_heads,\n",
        "        \"ff_dim\": self.ff_dim,\n",
        "        \"dropout_rate\": self.dropout_rate,\n",
        "    }\n",
        "  )\n",
        "  return config"
      ],
      "metadata": {
        "id": "-Tbdp-XtPvVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2VsWoHfZSOsQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}