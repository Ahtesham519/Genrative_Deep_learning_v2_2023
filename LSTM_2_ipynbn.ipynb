{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1s94BV5TYX6KYM4I1nyGj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ahtesham519/Genrative_Deep_learning_v2_2023/blob/main/LSTM_2_ipynbn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LSTM on Recipe DATA"
      ],
      "metadata": {
        "id": "sdDZfZ9TT8tk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFoiIyZNT2Z1"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import numpy as np\n",
        "import json\n",
        "import string\n",
        "import re\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks, losses"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#0 . Parameters"
      ],
      "metadata": {
        "id": "XqPjhLtJUU05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = 10000\n",
        "MAX_LEN= 200\n",
        "EMBEDDING_DIM = 100\n",
        "N_UNITS = 128\n",
        "VALIDATION_SPLIT = 0.2\n",
        "SEED = 42\n",
        "LOAD_MODEL = False\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 25\n"
      ],
      "metadata": {
        "id": "QxJRlbiZUTZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Load the data"
      ],
      "metadata": {
        "id": "-T4GLA3WUwbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load the full dataset\n",
        "with open(\"/app/data/epirecipes/full_format_recipes.json\") as json_data:\n",
        "  recipe_data = json.load(json_data)"
      ],
      "metadata": {
        "id": "wyfqK-NNUvF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Filter the dataset\n",
        "filtered_data = [\n",
        "    \"Recipe for \" + x[\"title\"] + \" | \" + \" \".join(x[\"directions\"])\n",
        "    for x in recipe_data\n",
        "    if \"title\" in x\n",
        "    and x[\"title\"] is not None\n",
        "    and \"directions\" in x\n",
        "    and x[\"directions\"] is not None\n",
        "]"
      ],
      "metadata": {
        "id": "FrM4XH_WVB_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Count the recipes\n",
        "n_recipes = len(filtered_data)\n",
        "print(f\"{n_recipes} recipes loaded\")"
      ],
      "metadata": {
        "id": "Zhh7TwdPVcnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = filtered_data[0]\n",
        "print(example)"
      ],
      "metadata": {
        "id": "djo-gx0TVlhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Tokenise the data"
      ],
      "metadata": {
        "id": "zPS83ru0Vrpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Pad the punctuation to treat them as seperate 'words'\n",
        "\n",
        "def pad_punctuation(s):\n",
        "  s = re.sub(f\"([{string.punctuation}])\", r\" \\1\" , s)\n",
        "  s = re.sub(\" + \" , \" \", s)\n",
        "  return s\n",
        "\n",
        "text_data = [pad_punctuation(x) for x in filtered_data]"
      ],
      "metadata": {
        "id": "vuYw6glKVp-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#display an example of a recipe\n",
        "example_data = text_data[9]\n",
        "example_data\n"
      ],
      "metadata": {
        "id": "O9NuhZcTWJWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert to a tensorflow dataset\n",
        "text_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices(text_data)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .shuffle(1000)\n",
        ")"
      ],
      "metadata": {
        "id": "Sa_f6M-cWQw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a vectorization layers\n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    standardize = \"lower\",\n",
        "    max_tokens = VOCAB_SIZE ,\n",
        "    output_mode = \"int\",\n",
        "    output_sequence_length = MAX_LEN + 1,\n",
        ")"
      ],
      "metadata": {
        "id": "CN7WG2DHWc9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Adapt the layer to the training set\n",
        "vectorize_layer.adapt(text_ds)\n",
        "vocab = vectorize_layer.get_vocabulary()"
      ],
      "metadata": {
        "id": "U7TzvNZ7WxgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Display some token:word mappings\n",
        "for i , word in enumerate(vocab[:10]):\n",
        "  print(f\"{i} : {word}\")"
      ],
      "metadata": {
        "id": "k_8xvVQwW55Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Display the same example converted to ints\n",
        "example_tokenised = vectorize_layer(example_data)\n",
        "print(example_tokenised.numpy())"
      ],
      "metadata": {
        "id": "FA7fN-XBXGBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Create the training set"
      ],
      "metadata": {
        "id": "AVTgZsEfXRSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create the training set of the recipes and the same text shifted by one word\n",
        "def prepare_inputs(text):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  tokenized_sentences = vectorize_layer(text)\n",
        "  x = tokenized_sentences[:, : -1]\n",
        "  y = tokenized_sentences[:, 1:]\n",
        "  return x , y\n",
        "\n",
        "train_ds = text_ds.map(prepare_inputs)"
      ],
      "metadata": {
        "id": "TRaSxA2lXPve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Build the LSTM"
      ],
      "metadata": {
        "id": "ZCGnXPmqX0Bm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = layers.Input(shape= (None,) , dtype = \"int32\")\n",
        "x = layers.Embedding(VOCAB_SIZE , EMBEDDING_DIM)(inputs)\n",
        "x = layers.LSTM(N_UNITS, return_sequences = True)(x)\n",
        "outputs = layers.Dense(VOCAB_SIZE , activation = \"softmax\")(x)\n",
        "lstm = model.Model(inputs, outputs)\n",
        "lstm.summary()"
      ],
      "metadata": {
        "id": "BnTzaJhFXzDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i7sMr1f6YRbR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}