{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPOilZVDqAHbHRjzZb0NIXF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ahtesham519/Genrative_Deep_learning_v2_2023/blob/main/Music_generation_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4Ovxphyt-GW"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers,models, losses,callbacks\n",
        "\n",
        "import music21"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#0. Parameters"
      ],
      "metadata": {
        "id": "kQOORsqWuYZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PARSE_MIDI_FILES = True\n",
        "PARSED_DATA_PATH = \"/app/notebooks/11_music/01_transformer/parsed_data/\"\n",
        "DATASET_REPETITIONS = 1\n",
        "\n",
        "SEQ_LEN = 50\n",
        "EMBEDDING_DIM = 50\n",
        "KEY_DIM = 256\n",
        "N_HEADS = 5\n",
        "DROPOUT_RATE = 0.3\n",
        "FEED_FORWARD_DIM = 256\n",
        "LOAD_MODEL = False\n",
        "\n",
        "#optimization\n",
        "EPOCHS = 50000\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "GENERATE_LEN = 50"
      ],
      "metadata": {
        "id": "_lbQRCt4uXrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Prepare the data"
      ],
      "metadata": {
        "id": "GlBOrHgIvBpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the data\n",
        "file_list = glob.glob(\"/app/data/bach-cello/*.mid\")\n",
        "print(f\"Found {len(file_list)} midi files\")"
      ],
      "metadata": {
        "id": "IalYTVo3vA29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = music21.converter"
      ],
      "metadata": {
        "id": "EB1fOy5lvTsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_score = (\n",
        "    music21.converter.parse(file_path[1]).splitAtQuarterLength(12)[0].chordify()\n",
        ")"
      ],
      "metadata": {
        "id": "kbviorkUvXBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_score.show()"
      ],
      "metadata": {
        "id": "Qa8MwJpQvihe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sxample_score.show(\"text\")"
      ],
      "metadata": {
        "id": "vu5vIkHpvkfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if PARSE_MIDI_FILES:\n",
        "  notes, durations = parse_midi_files(\n",
        "      file_list, parser, SEQ_LEN + 1 , PARSED_DATA_PATH\n",
        "  )\n",
        "else:\n",
        "  notes, durations = load_parsed_files()\n"
      ],
      "metadata": {
        "id": "5H0jQ9bGvm6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_notes = notes[658]\n",
        "example_durations = durations[658]\n",
        "print(\"\\nNotes string\\n\",example_notes,\"...\")\n",
        "print(\"\\nDuration string\\n\" , example_durations,\"...\")"
      ],
      "metadata": {
        "id": "fxOx95Rhv3ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Tokenize the Data"
      ],
      "metadata": {
        "id": "DZ7fIXpYwMF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(elements):\n",
        "  ds = (\n",
        "      tf.data.Dataset.from_tensor_slices(elements)\n",
        "      .batch(BATCH_SIZE, drop_remainder = True)\n",
        "      .shuffle(1000)\n",
        "  )\n",
        "  vectorizer_layer = layers.TextVectorization(\n",
        "      standardize = None , output_mode = \"int\"\n",
        "  )\n",
        "  vectorizer_layer.adapt(ds)\n",
        "  vocab = vectorize.get_vocabulary()\n",
        "  return ds, vectorizer_layer, vocab\n",
        "\n",
        "notes_seq_ds, notes_vectorize_layer, notes_vocab = create_dataset(notes)\n",
        "durations_seq_ds , durations_vectorize_layer, durations_vocab = create_dataset(\n",
        "    durations\n",
        ")\n",
        "seq_ds = tf.data.Dataset.zip((notes_seq_ds , durations_seq_ds))\n"
      ],
      "metadata": {
        "id": "62TwHLpUwKSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Display the same example notes and durations converted to ints\n",
        "example_tokenised_notes = notes_vectorize_layer(example_notes)\n",
        "example_tokenised_durations = durations_vectorize_layer(example_durations)\n",
        "print(\"{:10} {:10}\".format(\"note token\", \"duration token\"))\n",
        "for i , (note_int, duration_int) in enumerate(\n",
        "    zip(\n",
        "        example_tokenised_notes.numpy()[:11],\n",
        "        example_tokenised_durations.numpy()[:11],\n",
        "    )\n",
        "):\n",
        "    print(f\"{note_int:10}{duration_int:10}\")"
      ],
      "metadata": {
        "id": "tgzY5QwdxIW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "notes_vocab_size = len(notes_vocab)\n",
        "durations_vocab_size = len(durations_vocab)\n",
        "\n",
        "#Display some token:note mappings\n",
        "print(f\"\\nNOTES_VOCAB :length = {len(notes_vocab)}\")\n",
        "for i , note in enumerate(notes_vocab[:10]):\n",
        "  print(f\"{i}: {note}\")\n",
        "\n",
        "print(f\"\\nDURATIONS_VOCAB: length = {len(durations_vocab)}\")\n",
        "#Display some token:duration mappings\n",
        "for i , note in enumerate(durations_vocab[:10]):\n",
        "  print(f\"{i}:{note}\")"
      ],
      "metadata": {
        "id": "vizxWb_kx1lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Create the training Set"
      ],
      "metadata": {
        "id": "4SmrzwwVykjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the training set of sequences and the same sequences shifted by one note\n",
        "def prepare_inputs(notes, durations):\n",
        "  notes = tf.expand_dims(notes , -1)\n",
        "  durations = tf.expand_dims(durations , -1)\n",
        "  tokenized_notes = notes_vectorize_layer(notes)\n",
        "  tokenized_durations = durations_vectorize_layer(durations)\n",
        "  x = (tokenized_notes[:, : -1], tokenized_durations[:, :-1])\n",
        "  y = (tokenized_notes[:, 1:], tokenised_durations[:, 1:])\n",
        "  return x , y\n",
        "\n",
        "ds = seq_ds.map(prepare_inputs).repeat(DATASET_REPETITIONS)"
      ],
      "metadata": {
        "id": "KyXhuXq6yikA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_input_output = ds.take(1).get_single_element()\n",
        "print(example_input_output)"
      ],
      "metadata": {
        "id": "V78mFmLjzc3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#create the casual attention mask function"
      ],
      "metadata": {
        "id": "lq99cdB9rGjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def casual_attention_mask(batch_size, n_dest, n_src, dtype):\n",
        "  i = tf.range(n_dest)[:, None]\n",
        "  j = tf.range(n_src)\n",
        "  m = i >= j - n_src + n_dest\n",
        "  mask = tf.cast(m, dtype)\n",
        "  mask = tf.reshape(mask, [1, n_dest, n_src])\n",
        "  mult = tf.concat(\n",
        "      [tf.expand_dims(batch_size, -1), tf.constant([1,1] , dtype = tf.int32)], 0\n",
        "  )\n",
        "  return tf.tile(mask, mult)\n",
        "\n",
        "np.transpose(casual_attention_mask(1, 10, 10, dtype = tf.int32)[0])"
      ],
      "metadata": {
        "id": "r8TWZxBAzkUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create a Transformer Block Layer"
      ],
      "metadata": {
        "id": "b-KKcKGYr2Gd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(layers.layer):\n",
        "  def __init__(\n",
        "      self,\n",
        "      num_heads,\n",
        "      key_dim,\n",
        "      embed_dim,\n",
        "      ff_dim,\n",
        "      name,\n",
        "      dropout_rate = DROPOUT_RATE,\n",
        "  ):\n",
        "      super(TransformerBlock , self).__init__(name = name)\n",
        "      self.num_heads = num_heads\n",
        "      self.key_dim = key_dim\n",
        "      self.embed_dim = embed_dim\n",
        "      self.ff_dim = ff_dim\n",
        "      self.dropout_rate = dropout_rate\n",
        "      self.attn = layers.MultiHeadAttention(\n",
        "          num_heads , key_dim , output_shape = embed_dim\n",
        "      )\n",
        "      self.dropout_1 = layers.Dropout(self.dropout_rate)\n",
        "      self.ln_1 = layers.LayerNormalization(epsilon = 1e-6)\n",
        "      self.ffn_1 = layers.Dense(self.ff_dim , activation = \"relu\")\n",
        "      self.ffn_2 = layers.Dense(self.embed_dim)\n",
        "      self.dropout_2 = layers.Dropout(self.dropout_rate)\n",
        "      self.ln_2 = layers.LayerNormalization(epsilon = 1e - 6)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    input_shape = tf.shape(inputs)\n",
        "    batch_size = input_shape[0]\n",
        "    seq_len = input_shape[0]\n",
        "    casual_mask = casual_attention_mask(\n",
        "        batch_size, seq_len , seq_len , tf.bool\n",
        "    )\n",
        "    attention_output , attention_scores = self.attn(\n",
        "        inputs,\n",
        "        inputs,\n",
        "        attention_mask = casual_mask,\n",
        "        return_attention_scores = True,\n",
        "    )\n",
        "    attention_output = self.dropout_1(attention_output)\n",
        "    out1 = self.ln_1(inputs + attention_output)\n",
        "    ffn_1 = self.ffn_1(out1)\n",
        "    ffn_2 = self.ffn_2(ffn_1)\n",
        "    ffn_output = self.dropout_2(ffn_2)\n",
        "    return (self.ln_2(out1 + ffn_output), attention_scores)\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update(\n",
        "        {\n",
        "            \"key_dim\": self.key_dim,\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\":self.num_heads,\n",
        "            \"ff_dim\": self.ff_dim,\n",
        "            \"dropout_rate\" : self.dropout_rate,\n",
        "        }\n",
        "    )\n",
        "    return config"
      ],
      "metadata": {
        "id": "ZEFULE1gr1Kt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. Create the token and Position Embedding"
      ],
      "metadata": {
        "id": "CHkPK6H5uYmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "  def __init__(self, vocab_size , embed_dim):\n",
        "    super(TokenAndPositionEmbedding, self).__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embed_dim = embed_dim\n",
        "    self.token_emb = layers.Embedding(\n",
        "        input_dim = vocab_size ,\n",
        "        output_dim = embed_dim,\n",
        "        embeddings_initializer = \"he_uniform\",\n",
        "    )\n",
        "    self.pos_emb = SinePositionEmbedding()\n",
        "\n",
        "  def call(self, x):\n",
        "    embedding = self.token_emb(x)\n",
        "    positions = self.pos_emb(embedding)\n",
        "    return embedding + positions\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update(\n",
        "        {\n",
        "            \"vocab_size\" : self.vocab_size,\n",
        "            \"embed_dim\" : self.embed_dim,\n",
        "\n",
        "        }\n",
        "    )\n",
        "    return config"
      ],
      "metadata": {
        "id": "AxYp-LJ7uXMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tpe = TokenAndPositionEmbedding(notes_vocab_size , 32)\n",
        "token_embedding = tpe.token_emb(example_tokenised_notes)\n",
        "position_embedding = tpe.pos_emb(token_embedding)\n",
        "embedding = tpe(example_tokenised_notes)\n",
        "plt.imshow(\n",
        "    np.transpose(token_embedding),\n",
        "    cmap = \"coolwarm\",\n",
        "    interpolation = \"nearest\",\n",
        "    origin = \"lower\",\n",
        ")\n",
        "plt.show()\n",
        "plt.imshow(\n",
        "    np.transpose(position_embedding),\n",
        "    cmap = \"coolwarm\",\n",
        "    interpolation = \"nearest\",\n",
        "    origin = \"lower\",\n",
        ")\n",
        "plt.show()\n",
        "plt.imshow(\n",
        "    np.transpose(embedding),\n",
        "    cmap = \"coolwarm\",\n",
        "    interpolation = \"nearest\",\n",
        "    origin = \"lower\",\n",
        ")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VkS-Haxhvrtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. Build the Transformer model"
      ],
      "metadata": {
        "id": "mlUcLPD9wjkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IH-5ILnFwiiE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}