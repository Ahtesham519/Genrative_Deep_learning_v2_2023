{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyME8b3oa+zPDioKPhXUt/yU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ahtesham519/Genrative_Deep_learning_v2_2023/blob/main/Transformer_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models , losses , callbacks\n",
        "\n",
        "import music21\n",
        "\n",
        "from transformer_utils import(\n",
        "    parse_midi_files,\n",
        "    load_parsed_files ,\n",
        "    get_midi_note,\n",
        "    SinePositionEncoding,\n",
        ")"
      ],
      "metadata": {
        "id": "-fjuapkgdmCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#0. Parameters"
      ],
      "metadata": {
        "id": "8lmBAlCqeCXy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EkirauCdhjm"
      },
      "outputs": [],
      "source": [
        "PARSE_MIDI_FILES = True\n",
        "PARSED_DATA_PATH = \"/app/notebooks/11_music/01_transformer/parsed_data/\"\n",
        "DATASET_REPETITIONS = 1\n",
        "\n",
        "SEQ_LEN = 50\n",
        "EMBEDDING_DIM = 256\n",
        "KEY_DIM = 256\n",
        "N_HEADS = 5\n",
        "DROPOUT_RATE = 0.3\n",
        "FEED_FORWARD_DIM = 256\n",
        "LOAD_MODEL = False\n",
        "\n",
        "#optimization\n",
        "EPOCHS = 5000\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "GENERATE_LEN = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Prepare the Data"
      ],
      "metadata": {
        "id": "ljwbl5KiekUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the data\n",
        "file_path = glob.glob(\"/app/data/bach-cello/*.mid\")\n",
        "print(f\"Found {len(file_list)} midi files\")"
      ],
      "metadata": {
        "id": "pJkYCVrMejn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = music21.converter\n"
      ],
      "metadata": {
        "id": "r1XcZ7XJeyL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_score = (\n",
        "    music21.converter.parse(file_list[1]).splitAtQuarterLength(12)[0].chordify()\n",
        ")"
      ],
      "metadata": {
        "id": "vrv32Jrge0WV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_score.show()"
      ],
      "metadata": {
        "id": "lnntdnEme_fV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_score.show(\"text\")"
      ],
      "metadata": {
        "id": "vQ8QaFedfBof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if PARSE_MIDI_FILES:\n",
        "  notes, durations = parse_midi_files(\n",
        "      file_list, parser, SEQ_LEN + 1, PARSED_DATA_PATH\n",
        "  )\n",
        "else:\n",
        "  notes, durations = load_parsed_files()\n",
        ""
      ],
      "metadata": {
        "id": "wOwLR82pfD8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_notes = notes[658]\n",
        "example_durations = durations[658]\n",
        "print(\"\\nNotes string\\n\" , example_notes, \"...\")\n",
        "print(\"\\nDuration string\\n\", example_durations , \"...\")"
      ],
      "metadata": {
        "id": "3smBmtsxfSX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Tokenize the data"
      ],
      "metadata": {
        "id": "SgmImss8fkV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(elements):\n",
        "  ds = (\n",
        "      tf.data.Dataset.from_tensor_slices(elements)\n",
        "      .batch(BATCH_SIZE , drop_remainder = True)\n",
        "      .shuffle(1000)\n",
        "  )\n",
        "  vectorize_layer = layers.TextVectorization(\n",
        "      standardize = None, output_mode = \"int\"\n",
        "  )\n",
        "  vectorize_layer.adapt(ds)\n",
        "  vocab = vectorize_layer.get_vocabulary()\n",
        "  return ds , vectorize_layer, vocab\n",
        "\n",
        "notes_seq_ds,  notes_vectorize_layer, notes_vocab = create_dataset(notes)\n",
        "durations_seq_ds , durations_vectorize_layer , durations_vocab = create_dataset(\n",
        "    durations\n",
        ")\n",
        "seq_ds = tf.data.Dataset.zip((notes_seq_ds, durations_seq_ds))"
      ],
      "metadata": {
        "id": "YX3ijnaEfjpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Display the same example notes and durations converted to ints\n",
        "example_tokenised_notes = notes_vectorize_layer(example_notes)\n",
        "example_tokenised_durations = durations_vectorize_layer(example_durations)\n",
        "print(\"{:10} {:10}\".format(\"note token\" , \"duration token\"))\n",
        "for i , (note_int , duration_int) in enumerate(\n",
        "    zip(\n",
        "        example_tokenised_notes.numpy()[:11],\n",
        "        example_tokenised_durations.numpy()[:11]\n",
        "    )\n",
        "):\n",
        "    print(f\"{note_int:10}{duration_int:10}\")"
      ],
      "metadata": {
        "id": "lKrXxpoQgYdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "notes_vocab_size = len(notes_vocab)\n",
        "durations_vocab_size = len(durations_vocab)\n",
        "\n",
        "#Display some token:note mappings\n",
        "print(f\"\\nNOTES_VOCAB:length = {len(notes_vocab)}\")\n",
        "for i , note in enumerate(notes_vocab[:10]):\n",
        "  print(f\"{i}: {note}\")\n",
        "\n",
        "print(f\"\\nDURATIONS_VOCAb:length = {len(durations_vocab)}\")\n",
        "#Disaply some token:duration mappings\n",
        "for i , note in enumerate(durations_vocab[:10]):\n",
        "  print(f\"{i}: {note}\")\n",
        ""
      ],
      "metadata": {
        "id": "Jkw7rCPIhDtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Create the Training set"
      ],
      "metadata": {
        "id": "Ve4LYz7YhoJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the training set of sequences and the samesequences shifted by one note\n",
        "\n",
        "def prepare_inputs(notes, durations):\n",
        "  notes = tf.expand_dims(notes, -1)\n",
        "  durations = tf.expand_dims(durations , -1)\n",
        "  tokenized_notes = notes_vectorize_layer(notes)\n",
        "  tokenized_durations = durations_vectorize_layer(durations)\n",
        "  x = (tokenized_notes[:,:-1] , tokenized_durations[:, : -1])\n",
        "  y = (tokenized_notes[:, 1:], tokenized_durations[:, 1:])\n",
        "  return x , y\n",
        "\n",
        "ds = seq_ds.map(prepare_inputs).repeat(DATASET_REPETITIONS)"
      ],
      "metadata": {
        "id": "yAb5vqvMhncK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_input_output = ds.take(1).get_single_element()\n",
        "print(example_input_output)"
      ],
      "metadata": {
        "id": "oQHokAWGiZQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Create the casual attention mask function"
      ],
      "metadata": {
        "id": "cbwJ-xOdiiYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def casual_attention_mask(batch_size , n_dest,  n_src , dtype):\n",
        "  i = tf.range(n_dest)[:, None]\n",
        "  j = tf.range(n_src)\n",
        "  m = i >= j - n_src + n_dest\n",
        "  mask = tf.cast(m, dtype)\n",
        "  mask = tf.reshape(mask , [1, n_dest , n_src])\n",
        "  mult = tf.concat(\n",
        "      [tf.expand_dims(batch_size , -1), tf.constant([1,1], dtype= tf.int32)], 0\n",
        "  )\n",
        "  return tf.title(mask, mult)\n",
        "\n",
        "np.transpose(casual_attention_mask(1, 10, 10, dtype = tf.int32)[0])"
      ],
      "metadata": {
        "id": "wNxr4XT4ihqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Create a Trandformer Block layer"
      ],
      "metadata": {
        "id": "bGfywN8VjOPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "  def __init__(\n",
        "      self,\n",
        "      num_heads,\n",
        "      key_dim,\n",
        "      embed_dim ,\n",
        "      ff_dim,\n",
        "      name,\n",
        "      dropout_rate = DROPOUT_RATE,\n",
        "\n",
        "  ):\n",
        "  super(TransformerBlock, self).__init__(name = name)\n",
        "  self.num_heads = num_heads\n",
        "  self.key_dim = key_dim\n",
        "  self.embed_dim = embed_dim\n",
        "  self.ff_dim = ff_dim\n",
        "  self.dropout_rate = dropout_rate\n",
        "  self.attn = layers.MultiHeadAttention(\n",
        "      num_heads, key_dim , output_shape = embed_dim\n",
        "  )\n",
        "  self.dropout_1 = layers.Dropout(self.dropout_rate)\n",
        "  self.ln_1 = layers.LayerNormalization(eplison = 1e-6)\n",
        "  self.ffn_1 = layers.Dense(self.ff_dim , activation = \"relu\")\n",
        "  self.ffn_2 = layers.Dense(self.embed_dim)\n",
        "  self.dropout_2 = layers.Dropout(self.dropout_rate)\n",
        "  self.ln_2 = layers.LayerNormalization(epsilon = 1e-6)\n",
        "\n",
        "def call(self, inputs ):\n",
        "  input_shape = tf.shape(inputs)\n",
        "  batch_size = input_shape[0]\n",
        "  seq_len = input_shape[1]\n",
        "  casual_mask = casual_attention_mask(\n",
        "      batch_size , seq_len, seq_len, tf.bool\n",
        "  )\n",
        "  attention_output , attention_scores = self.attn(\n",
        "      inputs,\n",
        "      inputs,\n",
        "      attention_mask = casual_mask ,\n",
        "      return_attention_scores = True,\n",
        "  )\n",
        "  attention_ouput = self.dropout_1(attention_output)\n",
        "  out1 = self.ln_1(inputs + attention_output)\n",
        "  ffn_1 = self.ffn_1(out1)\n",
        "  ffn_2 = self.ffn_2(ffn_1)\n",
        "  ffn_output = self.dropout_2(ffn_2)\n",
        "  return (self.ln_2(out1 + ffn_output) , attention_scores)\n",
        "\n",
        "def get_config(self):\n",
        "  config = super().get_config()\n",
        "  config.update(\n",
        "      {\n",
        "          \"key_dim\": self.key_dim,\n",
        "          \"embed_dim\": self.embed_dim,\n",
        "          \"num_heads\": self.num_heads,\n",
        "          \"ff_dim\": self.ff_dim,\n",
        "          \"dropout_rate\": self.dropout_rate,\n",
        "      }\n",
        "  )\n",
        "  return config"
      ],
      "metadata": {
        "id": "-8q4RnfcjM4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. Create the token and position Embedding"
      ],
      "metadata": {
        "id": "7Gvx1NcQGOzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "  def __init__(self, vocab_size , embed_dim):\n",
        "    super(TokenAndPositionEmbedding , self).__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embed_dim = embed_dim\n",
        "    self.token_emb = layers.Embedding(\n",
        "        input_dim = vocab_size ,\n",
        "        output_dim = embed_dim ,\n",
        "        embeddings_initializer = \"he_uniform\",\n",
        "    )\n",
        "    self.pos_emb = SinePositionEncoding()\n",
        "\n",
        "  def call(self, x):\n",
        "    embedding = self.token_emb(x)\n",
        "    positions = self.pos_emb(embedding)\n",
        "    return embedding + positions\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update(\n",
        "        {\n",
        "            \"vocab_size \": self.vocab_size ,\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "        }\n",
        "    )\n",
        "    return config"
      ],
      "metadata": {
        "id": "FUlALXH9GNw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tpe = TokenAndPositionEmbedding(notes_vocab_size , 32)\n",
        "token_embedding = tpe.token_emb(example_tokenised_notes)\n",
        "position_embedding = tpe.pos_emb(token_embedding)\n",
        "embedding = tpe(example_tokenised_notes)\n",
        "plt.imshow(\n",
        "    np.transpose(token_embedding),\n",
        "    cmap = \"coolwarm\",\n",
        "    interpolation = \"nearest\",\n",
        "    origin = \"lower\",\n",
        ")\n",
        "plt.show()\n",
        "plt.imshow(\n",
        "np.transpose(position_embedding),\n",
        "cmap = \"coolwarm\",\n",
        "interpolation = \"nearest\",\n",
        "origin = \"lower\",\n",
        ")\n",
        "plt.show()\n",
        "plt.imshow(\n",
        "    np.transpose(embedding),\n",
        "    cmap = \"coolwarm\",\n",
        "    interpolation = \"nearest\",\n",
        "    origin = \"lower\",\n",
        ")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tLHGG_8lHgOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. Bulid the Transformer model"
      ],
      "metadata": {
        "id": "NJQOkKb9IeLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "note_inputs = layers.Input(shape = (None,), dtype = tf.int32)\n",
        "durations_inputs = layers.Input(shape = (None,), dtype = tf.int32)\n",
        "note_embeddings = TokenAndPositionEmbedding(\n",
        "    notes_vocab_size , EMBEDDING_DIM // 2\n",
        ")(note_inputs)\n",
        "duration_embeddings = TokenAndPositionEmbedding(\n",
        "    durations_vocab_size , EMBEDDING_DIM //2\n",
        ")(durations_inputs)\n",
        "embeddings = layers.Concatenate()([note_embeddings, duration_embeddings])\n",
        "x, attention_scores = TransformerBlock(\n",
        "    N_HEADS , KEY_DIM , EMBEDDING_DIM , FEED_FORWARD_DIM , name = \"attention\"\n",
        ")(embeddings)\n",
        "note_outputs = layers.Dense(\n",
        "    notes_vocab_size , activation = \"softmax\", name = \"note_outputs\"\n",
        ")(x)\n",
        "duration_outputs = layers.Dense(\n",
        "    durations_vocab_size , activation = \"softmax\", name = \"duration_outputs\"\n",
        ")(x)\n",
        "model = models.Model(\n",
        "    inputs = [note_inputs , durations_inputs],\n",
        "    outputs = [note_outputs , duration_outputs ],\n",
        ")\n",
        "model.compile(\n",
        "    \"adam\",\n",
        "    loss = [\n",
        "        losses.SparseCategoricalCrossentropy(),\n",
        "        losses.SparseCategoricalCrossentropy(),\n",
        "    ],\n",
        ")\n",
        "att_model = models.Model(\n",
        "    inputs = [note_inputs , durations_inputs], outputs = attention_scores\n",
        ")\n"
      ],
      "metadata": {
        "id": "TEllkEVLIdEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "J5mgOn2EKPDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if LOAD_MODEL:\n",
        "  model.load_weights(\"./checkpoint/checkpoint/ckpt\")\n",
        ""
      ],
      "metadata": {
        "id": "Trpxw754KQYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9. Train the transformer"
      ],
      "metadata": {
        "id": "IcLMJqRIKcuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a Music Generator checkpoint\n",
        "class MusicGenerator(callbcaks.Callback):\n",
        "  def __init__(self, index_to_note, index_to_duration, top_k = 10):\n",
        "    self.index_to_note = index_to_note\n",
        "    self.note_to_index = {\n",
        "        note: index for index , note in enumerate(index_to_note)\n",
        "    }\n",
        "    self.index_to_duration = index_to_duration\n",
        "    self.duration_to_index = {\n",
        "        duration: index for index , duration in enumerate(index_to_duration)\n",
        "    }\n",
        "\n",
        "  def sample_from(self , probs , temperature):\n",
        "    probs = probs ** (1 / temperature)\n",
        "    probs = probs / np.sum(probs)\n",
        "    return np.random.choice(len(probs) , p = probs), probs\n",
        "\n",
        "  def get_note(self , notes , durations , temperature):\n",
        "    sample_note_idx = 1\n",
        "    while = sample_note_idx == 1:\n",
        "      sample_note_idx , note_probs = self.sample_from(\n",
        "        notes[0][-1] , temperature\n",
        "    )\n",
        "      sample_note = self.index_to_note[sample_note_idx]\n",
        "\n",
        "    new_note = get_midi_note(sample_note , smaple_duration)\n",
        "\n",
        "    return(\n",
        "        new_note,\n",
        "        sample_note_idx,\n",
        "        sample_note,\n",
        "        note_probs,\n",
        "        smaple_duration_idx,\n",
        "        sample_duration,\n",
        "        duration_probs,\n",
        "    )\n",
        "\n",
        "  def generate(self, start_notes, start_durations , max_tokens ,  temperature):\n",
        "    attention_model = models.Model(\n",
        "        inputs = self.model.input,\n",
        "        outputs = self.model.get_layer(\"attention\").output,\n",
        "    )\n",
        "\n",
        "    start_note_tokens  = [self.note_to_index.get(x, 1) for x in start_notes]\n",
        "    start_duration_tokens = [\n",
        "        self.duration_to_index.get(x, 1) for x in start_durations\n",
        "    ]\n",
        "    sample_note = None\n",
        "    sample_duration = None\n",
        "    info = []\n",
        "    midi_stream = music21.stream.Stream()\n",
        "\n",
        "    midi_stream.append(music21.clef.BassClef())\n",
        "\n",
        "    for smaple_note, sample_duration in zip(start_notes , start_durations):\n",
        "      new_note = get_midi_note(sample_note, sample_duration)\n",
        "      if new_note is not None:\n",
        "        midi_stream.append(new_note)\n",
        "\n",
        "    while len(start_note_tokens) < max_tokens:\n",
        "      x1 = np.array([start_note_tokens])\n",
        "      x2 = np.array([start_duration_tokens])\n",
        "      notes, durations = self.model.predict([x1, x2], verbose = 0)\n",
        "\n",
        "      repeat = True\n",
        "\n",
        "      while repeat:\n",
        "        (\n",
        "            new_note,\n",
        "            sample_note_idx,\n",
        "            sample_note,\n",
        "            note_probs,\n",
        "            sample_duration_idx,\n",
        "            sample_duration,\n",
        "            duration_probs,\n",
        "        ) = self.get_note(notes, durations , temperature)\n",
        "\n",
        "        if (\n",
        "            isinstance(new_note, music21.chord.Chord)\n",
        "            or isinstance(new_note, music21.note.Note)\n",
        "            or isinstance(new_note, music21.note.Note)\n",
        "        ) and sample_duration == \"0.0\":\n",
        "            repeat = True\n",
        "        else:\n",
        "          repeat = False\n",
        "\n",
        "      if new_note is note None:\n",
        "        midi_stream.append(new_note)\n",
        "\n",
        "      _, att = attention_model.predict([x1, x2], verbose = 0)\n",
        "\n",
        "      info.append(\n",
        "          {\n",
        "              \"prompt\": [start_notes.copy(), start_durations.copy()],\n",
        "              \"midi\" : midi_stream,\n",
        "              \"chosen_note\": (sample_note , sample_duration),\n",
        "              \"note_probs\": note_probs,\n",
        "              \"duration_probs\": duration_probs,\n",
        "              \"atts\" :att[0, : , -1, :],\n",
        "          }\n",
        "      )\n",
        "      start_note_tokens.append(sample_note_idx)\n",
        "      start_duration_tokens.append(sample_duration_idx)\n",
        "      start_notes.append(sample_note)\n",
        "      start_durations.append(sample_duration)\n",
        "\n",
        "\n",
        "      if sample_note == \"START\":\n",
        "        break\n",
        "\n",
        "  return info\n",
        "\n",
        "def on_epoch_end(self, epoch, logs = None):\n",
        "  info = self.generate(\n",
        "      [\"START\"], [\"0.0\"], max_tokens = GENERATE_LEN, temperature = 0.5\n",
        "  )\n",
        "  midi_stream = info[-1][\"midi\"].chordify()\n",
        "  print(info[-1][\"prompt\"])\n",
        "  midi_stream.show()\n",
        "  midi_stream.write(\n",
        "      \"midi\",\n",
        "      fp = os.path.join(\n",
        "          \"/app/notebook/11_music/01_transofrmer/output\",\n",
        "          \"output-\" + str(epoch).zfill(4) + \".md\",\n",
        "      ),\n",
        "  )\n"
      ],
      "metadata": {
        "id": "inBk8Ca_Kbyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a model save checkpoint\n",
        "model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
        "    filepath = \"./checkpoint/checkpoint.ckpt\",\n",
        "    save_weights_only = True,\n",
        "    save_freq = \"epoch\",\n",
        "    verbose = 0,\n",
        ")\n",
        "\n",
        "tensorboard_callback = callbacks.Tensorboard(log_dir = \"./logs\")\n",
        "\n",
        "#Tokenize starting prompt\n",
        "music_generator = MusicGenerator(notes_vocab , durations_vocab)"
      ],
      "metadata": {
        "id": "cnuYrvLyqhxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    ds,\n",
        "    epochs = EPOCHS,\n",
        "    callbacks= [\n",
        "        model_checkpoint_callback,\n",
        "        tensorboard_callback,\n",
        "        music_generator,\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "K4qog2rYq--P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save the final model\n",
        "model.save(\"./models/model\")"
      ],
      "metadata": {
        "id": "xmFvHZGIrKnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Generate music using the transformer"
      ],
      "metadata": {
        "id": "WAGxDG7OrWqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "info = music_generator.generate(\n",
        "    [\"START\"], [\"0.0\"], max_tokens = 50, temperature = 0.5\n",
        ")\n",
        "midi_stream = info[-1][\"midi\"].chordify()\n",
        "midi_stream.show()\n"
      ],
      "metadata": {
        "id": "MKzT9o9YrVve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Write music to MIDI file"
      ],
      "metadata": {
        "id": "NV1etk9vrpHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "timestr = time.strtime(\"%Y%m%d-%H%M%S\")\n",
        "midi_stream.write(\n",
        "    \"midi\",\n",
        "    fp = os.path.join(\n",
        "        \"/app/notebook/11_music/01_transformer/output\",\n",
        "        \"output-\" + timestr + \"midi\",\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "j_ey-n0hrobO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Note probabilities"
      ],
      "metadata": {
        "id": "egw5b63BsAVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_pitch = 70\n",
        "seq_len = len(info)\n",
        "grid = np.zeros((max_pitch,seq_len), dtype = np.float32)\n",
        "\n",
        "for j in range(seq_len):\n",
        "  for i , prob in enumerate(info[j][\"note_probs\"]):\n",
        "    try:\n",
        "      pitch = music21.note.Note(notes_vocab[i]).pitch.midi\n",
        "      grid[pitch, j] = prob\n",
        "    except:\n",
        "      pass"
      ],
      "metadata": {
        "id": "xf1UeY9Hr_gA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig , ax = plt.subplots(figsize = (8,8))\n",
        "ax.set_yticks([int(j) for j in range(35 , 70)])\n",
        "plt.imshow(\n",
        "    grid[35:70, :],\n",
        "    origin = \"lower\",\n",
        "    cmap = \"coolwarm\",\n",
        "    vmin = -0.5,\n",
        "    vmax = 0.5,\n",
        "    extent = [0, seq_len, 35, 70],\n",
        "    )\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QOfP58-Gsg7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Attention Plot"
      ],
      "metadata": {
        "id": "bvPHK5_2s6d4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_size = 20\n",
        "\n",
        "att_matrix = np.zeros((plot_size , plot_size))\n",
        "prediction_output = []\n",
        "last_prompt = []"
      ],
      "metadata": {
        "id": "gLY8AKtes5oP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for j in range(plot_size):\n",
        "  atts = info[j][\"atts\"].max(axis = 0)\n",
        "  att_matrix[: (j + 1 ), j] = atts\n",
        "  prediction_output.append(info[j][\"chosen_note\"][0])\n",
        "  last_prompt.append(info[j][\"prompt\"][0][-1])"
      ],
      "metadata": {
        "id": "uBIcL6swtDzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig , ax = plt.suubplots(figsize = (8,8))\n",
        "im = ax.imshow(att_matrix, cmap = \"Greens\", interpolation = \"neareast\")\n",
        "\n",
        "ax.set_xticks(np.arange(-0.5, plot_size , 1) , minor = True)\n",
        "ax.set_yticks(np.arange(-0.5, plot_size , 1) , minor = True)\n",
        "ax.grid(which= \"minor\" , color = \"black\" , linestyle = \"-\" , linewidth = 1)\n",
        "ax.set_xticks(np.arange(plot_size))\n",
        "ax.set_yticks(np.arange(plot_size))\n",
        "ax.set_xticklabels(prediction_output[:plot_size])\n",
        "ax.set_yticklabels(last_prompt[:plot_size])\n",
        "ax.xaxis.tick_top()\n",
        "\n",
        "plt.setp(\n",
        "    ax.get_xticklabels(),\n",
        "    rotation = 90,\n",
        "    ha = \"left\",\n",
        "    va = \"center\",\n",
        "    rotation_mode = \"anchor\",\n",
        ")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "f91kVWkhtlzy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}