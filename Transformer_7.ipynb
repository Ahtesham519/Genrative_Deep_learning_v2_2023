{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNih2o7yuHPFw090RUuGZsH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ahtesham519/Genrative_Deep_learning_v2_2023/blob/main/Transformer_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwGjIWBBDlkq"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import time\n",
        "import matlplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from  tensorflow.keras import layers, models, losses, callbacks\n",
        "\n",
        "import music21\n",
        "\n",
        "from tensorflow_utils import(\n",
        "    parse_midi_files,\n",
        "    load_parsed_files,\n",
        "    get_midi_note,\n",
        "    SinePositionEncoding,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#0. Parameters"
      ],
      "metadata": {
        "id": "s40bNn-QEG7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PARSE_MIDI_FILES = True\n",
        "PARSED_DATA_PATH = \"/app/notebooks/11_music/01_transformer/parsed_data/\"\n",
        "DATASET_REPETITIONS = 1\n",
        "\n",
        "SEQ_LEN = 50\n",
        "EMBEDDING_DIM = 256\n",
        "KEY_DIM = 256\n",
        "N_HEADS = 5\n",
        "DROPOUT_RATE = 0.3\n",
        "FEED_FORWARD_DIM = 256\n",
        "LOAD_MODEL = False\n",
        "\n",
        "#optimization\n",
        "EPOCHS = 5000\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "GENERATE_LEN = 50"
      ],
      "metadata": {
        "id": "l-iyD0dxEGGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Prepare the Data"
      ],
      "metadata": {
        "id": "j9YEhhYwEmKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load the data\n",
        "file_list = glob.glob(\"/app/data/bach-cello/*.mid\")\n",
        "print(f\"Found {len(file_list)} midi files\")"
      ],
      "metadata": {
        "id": "KDPIDNjwElFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = music21.converter"
      ],
      "metadata": {
        "id": "3ext_Q-5E0bP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_score = (\n",
        "    music21.converter.parse(file_list[1].splitAtQuaterLength(12)[0].chordify())\n",
        ")"
      ],
      "metadata": {
        "id": "Mq-O0z_QE3cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_score.show()"
      ],
      "metadata": {
        "id": "YtQv4RDNFCD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_score.show(\"text\")"
      ],
      "metadata": {
        "id": "SHwWZKtBFEQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if PARSE_MIDI_FILES:\n",
        "  notes, durations = parse_midi_files(\n",
        "      file_list , parser, SEQ_LEN + 1, PARSED_DATA_PATH\n",
        "  )\n",
        "else:\n",
        "  notes, durations = load_parsed_files()"
      ],
      "metadata": {
        "id": "fMeXjVarFGew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_notes = notes[658]\n",
        "example_durations = durations[658]\n",
        "print(\"\\n Notes string \\n\", example_notes, \"...\")\n",
        "print(\"\\nDuration string\\n\", example_durations, \"...\")"
      ],
      "metadata": {
        "id": "j4PrYqmhFS0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Tokenize the data"
      ],
      "metadata": {
        "id": "RlpwPcI8Fnyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(elements):\n",
        "  ds = (\n",
        "      tf.data.Dataset.from_tensor_slices(elements)\n",
        "      .batch(BATCH_SIZE , drop_remainder = True)\n",
        "      .shuffle(1000)\n",
        "  )\n",
        "  vectorize_layer = layers.TextVectorization(\n",
        "      standardize = None , output_mode = \"int\"\n",
        "  )\n",
        "  vectorize_layer.adapt(ds)\n",
        "  vocab = vectorize_layer.get_vocabulary()\n",
        "  return ds, vectorize_layer , vocab\n",
        "\n",
        "notes_seq_ds, notes_vectorize_layer , notes_vocab = create_dataset(notes)\n",
        "durations_seq_ds , durations_vectorize_layer , durations_vocab = create_dataset(\n",
        "    durations\n",
        ")\n",
        "seq_ds = tf.data.Dataset.zip((notes_seq_ds, durations_seq_ds))"
      ],
      "metadata": {
        "id": "XX4yx9upFm9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Display the same example notes and durations converted to ints\n",
        "example_tokenised_notes = notes_vecotrize_layer(example_notes)\n",
        "example_tokenised_durations = durations_vectorize_layer(example_durations)\n",
        "print(\"{:10} {:10}\".format(\"note token\", \"duration token\"))\n",
        "for i , (note_int , duration_int) in enumerate(\n",
        "    zip(\n",
        "        example_tokenised_notes.numpy()[:11],\n",
        "        example_tokenised_durations.numpy()[:11],\n",
        "    )\n",
        "):\n",
        "    print(f\"{note_int:10}{duration_int:10}\")"
      ],
      "metadata": {
        "id": "VH6jip3PGXsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "notes_vocab_size = len(notes_vocab)\n",
        "durations_vocab_size = len(durations_vocab)\n",
        "\n",
        "#Display some token:note mappings\n",
        "print(f\"\\NOTES_VOCAB:lenght = {len(notes_vocab)}\")\n",
        "for i , note in enumerate(notes_vocab[:10]):\n",
        "  print(f\"{i}: {note}\")\n",
        "\n",
        "print(f\"\\nDURATIONS_VOCAB:length = {len(durations_vocab)}\")\n",
        "#Display some token:duration mappings\n",
        "for i , note in enumerate(durations_vocab[:10]):\n",
        "  print(f\"{i}: {note}\")"
      ],
      "metadata": {
        "id": "z7h9BDDNHAGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Create the training Set"
      ],
      "metadata": {
        "id": "VdHhNb5_HmfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the training set of sequences and the same sequences shifted by one note\n",
        "def prepare_inputs(notes, durations):\n",
        "  notes = tf.expand_dims(notes, -1)\n",
        "  durations = tf.expand_dims(durations , -1)\n",
        "  tokenized_notes = notes_vectorize_layer(notes)\n",
        "  tokenized_durations = durations_vectorize_layer(durations)\n",
        "  x = (tokenized_notes[:,:-1], tokenized_durations[:, :-1])\n",
        "  y = (tokenized_notes[:,1:], tokenized_durations[:,1:])\n",
        "  return x, y\n",
        "\n",
        "\n",
        "ds = seq_ds.map(prepare_inputs).repeat(DATASET_REPETITIONS)"
      ],
      "metadata": {
        "id": "1Q8jyNoWHFwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_input_output = ds.take(1).get_single_element()\n",
        "print(example_input_output)"
      ],
      "metadata": {
        "id": "kk7LGtKEIWkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Create the casual attention mask function"
      ],
      "metadata": {
        "id": "JZ9esi71JA10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def casual_attention_mask(batch_size , n_dest , n_src, dtype):\n",
        "  i = tf.range(n_dest)[:, None]\n",
        "  j = tf.range(n_src)\n",
        "  m = i >= j - n_src + n_dest\n",
        "  mask = tf.cast(m , dtype)\n",
        "  mask = tf.range(mask , [1, n_dest , n_src])\n",
        "  mult = tf.concat(\n",
        "      [tf.expand_dims(batch_size , -1), tf.constant([1,1], dtype = tf.int32)] 0\n",
        "  )\n",
        "  return tf.title(mask , mult)\n",
        "\n",
        "np.transpose(casual_attention_mask(1, 10, 10, dtype = tf.int32)[0])"
      ],
      "metadata": {
        "id": "lVjUkR6EI_uU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Create a Transformer Block layer"
      ],
      "metadata": {
        "id": "AmFv8eDNJsM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "  def __init__(\n",
        "      self,\n",
        "      num_heads,\n",
        "      key_dim,\n",
        "      embed_dim,\n",
        "      ff_dim,\n",
        "      name,\n",
        "      dropout_rate, = DROPOUT_RATE,\n",
        "  ):\n",
        "      super(TransformerBlock, self).__init__(name = name)\n",
        "      self.num_heads = num_heads\n",
        "      self.key_dim = key_dim\n",
        "      self.embed_dim = embed_dim\n",
        "      self.ff_dim = ff_dim\n",
        "      self.dropout_rate = dropout_rate\n",
        "      self.attn = layers.MultiHeadAttention(\n",
        "          num_heads , key_dim , output_shape = embed_dim\n",
        "      )\n",
        "      self.dropout_1 = layers.Dropout(self.dropout_rate)\n",
        "      self.ln_1 = layers.LayerNormalization(epsilon = 1e-6)\n",
        "      self.ffn_1 = layers.Dense(self.ff_dim , activation = \"relu\")\n",
        "      self.ffn_2 = layers.Dense(self.embed_dim)\n",
        "      self.dropout_2 = layers.Dropout(self.dropout_rate)\n",
        "      self.ln_2 = layers.LayerNormalization(epsilon = 1e-6)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    input_shape = tf.shape(inputs)\n",
        "    batch_size = input_shape[0]\n",
        "    seq_len = input_shape[0]\n",
        "    casual_mask = casual_attention_mask(\n",
        "        batch_size , seq_len , seq_len , tf.bool\n",
        "    )\n",
        "    attention_output , attention_scores = self.attn(\n",
        "        inputs,\n",
        "        inputs,\n",
        "        attention_mask = casual_mask,\n",
        "        return_attention_scores = True,\n",
        "    )\n",
        "    attention_output = self.dropout_1(attention_output)\n",
        "    out1 = self.ln_1(inputs + attention_output)\n",
        "    ffn_1 = self.ffn_1(out1)\n",
        "    ffn_2 = self.ffn_2(ffn_1)\n",
        "    ffn_output = self.dropout_2(ffn_2)\n",
        "    return (self.ln_2(out1 + ffn_output) , attention_scores)\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update(\n",
        "        {\n",
        "            \"key_dim\": self.key_dim,\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"ff_dim\": self.ff_dim,\n",
        "            \"dropout_rate\": self.dropout_rate,\n",
        "        }\n",
        "    )\n",
        "    return config\n"
      ],
      "metadata": {
        "id": "N0WBMoNxJrSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7 . Create the token and position Embedding"
      ],
      "metadata": {
        "id": "ElRPqbCLMBPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "  def __init__(self, vocab_size, embed_dim):\n",
        "    super(TokenAndPositionEmbedding, self).__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embed_dim = embed_dim\n",
        "    self.token_emb = layers.Embedding(\n",
        "        input_dim = vocab_size ,\n",
        "        output_dim = embed_dim ,\n",
        "        embeddings_initializer = \"he_uniform\",\n",
        "    )\n",
        "    self.pos_emb = SinePositionEncoding()\n",
        "\n",
        "  def call(self, x):\n",
        "    embedding = self.token_emb(x)\n",
        "    positions = self.pos_emb(embedding)\n",
        "    return embedding + positions\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update(\n",
        "        {\n",
        "            \"vocab_size\": self.vocab_size,\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "        }\n",
        "    )\n",
        "    return config"
      ],
      "metadata": {
        "id": "8op2gP5-MALg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tpe = TokenAndPositionEmbedding(notes_vocab_size , 32)\n",
        "token_embedding = tpe.token_emb(example_tokenised_notes)\n",
        "position_embedding = tpe.pos_emb(token_embedding)\n",
        "embedding = tpe(example_tokenised_notes)\n",
        "plt.imshow(\n",
        "    np.transpose(token_embedding),\n",
        "    cmap = \"coolwarm\",\n",
        "    interpolation  = \"nearest\",\n",
        "    origin = \"lower\",\n",
        ")\n",
        "plt.show()\n",
        "plt.imshow(\n",
        "    np.transpose(position_embedding),\n",
        "    cmap = \"coolwarm\",\n",
        "    interpolation = \"nearest\",\n",
        "    origin = \"lower\",\n",
        " )\n",
        "plt.show()\n",
        "plt.imshow(\n",
        "    np.transpose(embedding),\n",
        "    cmap = \"coolwarm\",\n",
        "    interpolation = \"nearest\",\n",
        "    origin = \"lower\",\n",
        ")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aLuF30KMM5B5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. Bulid the Transformer model"
      ],
      "metadata": {
        "id": "PoOcZZ0ENod8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "note_inputs = layers.Input(shape = (None,) , dtype = tf.int32)\n",
        "durations_inputs = layers.Input(shape = (None,), dtype = tf.int32)\n",
        "note_embeddings = TokenAndPositionEmbedding(\n",
        "    notes_vocab_size , EMBEDDING_DIM //2\n",
        ")(note_inputs)\n",
        "duration_embeddings = TokenAndPositionEmbedding(\n",
        "    durations_vocab_size , EMBEDDING_DIM // 2\n",
        ")(durations_inputs)\n",
        "embeddings = layers.Concatenate()([note_embeddings, duration_embeddings])\n",
        "x, attention_scores = TransformerBlock(\n",
        "    N_HEADS, KEY_DIM , EMBEDDING_DIM , FEED_FORWARD_DIM = name = \"attention\"\n",
        ")(embeddings)\n",
        "note_outputs = layers.Dense(\n",
        "    notes_vocab_size , activation = \"softmax\", name = \"note_outputs\"\n",
        ")(x)\n",
        "duration_outputs = layers.Dense(\n",
        "    durations_vocab_size ,activation = \"softmax\" , name = \"duration_output\"\n",
        ")(x)\n",
        "model = models.Model(\n",
        "    inputs = [note_inputs , durations_input],\n",
        "    outputs = [note_outputs , duration_outputs],\n",
        ")\n",
        "model.compile(\n",
        "    \"adam\",\n",
        "    loss = [\n",
        "        losses.SparseCategoricalCrossentropy(),\n",
        "        losses.SparseCategoricalCrossentropy(),\n",
        "    ],\n",
        ")\n",
        "att_model - model.Model(\n",
        "    inputs = [note_inputs, durations_inputs], outputs = attention_scores\n",
        ")\n"
      ],
      "metadata": {
        "id": "csloDQi7Nnvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "QQuZLcWHPD0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if LOAD_MODEL:\n",
        "  model.load_weights(\"./checkpoint/checkpoint.ckpt\")\n",
        ""
      ],
      "metadata": {
        "id": "79VzBKbdPGAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9. Train the Transofomer"
      ],
      "metadata": {
        "id": "2SvYQfjjPQ-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a MusicGenerate checkpoint\n",
        "class MusicGenerator(callbacks.Callback):\n",
        "  def __init__(self, index_to_note, index_to_durations, top_k = 10):\n",
        "    self.index_to_note = index_to_note\n",
        "    self.note_to_index = {\n",
        "        note: index for index  , note in enumerate(index_to_note)\n",
        "    }\n",
        "    self.index_to_duration = index_to_duration\n",
        "    self.duration_to_index = {\n",
        "        duration: index for index, duration in enumerate(index_to_duration)\n",
        "    }\n",
        "\n",
        "  def sample_from(self, probs , temperature):\n",
        "    probs = probs ** (1 / temperature)\n",
        "    probs = probs / np.sum(probs)\n",
        "    return np.random.choice(len(probs) , p = probs) , probs\n",
        "\n",
        "  def get_note(self, notes, durations , temperature):\n",
        "    sample_note_idx = 1\n",
        "    while sample_note_idx == 1:\n",
        "      sample_note_idx, note_probs = self.sample_from(\n",
        "          notes[0][-1], temperature\n",
        "      )\n",
        "      sample_note = self.index_to_note[sample_note_idx]\n",
        "\n",
        "    sample_duration_idx = 1\n",
        "    while sample_duration_idx == 1:\n",
        "      sample_duration_idx , duration_probs = self.sample_from(\n",
        "          durations[0][-1], temperature\n",
        "      )\n",
        "      sample_duration = self.index_to_duration[sample_duration_idx]\n",
        "\n",
        "    new_note = get_midi_note(sample_note, sample_duration)\n",
        "\n",
        "    return (\n",
        "        new_note,\n",
        "        sample_note_idx,\n",
        "        sample_note,\n",
        "        note_probs,\n",
        "        sample_duration_idx,\n",
        "        sample_duration,\n",
        "        duration_probs,\n",
        "    )\n",
        "\n",
        "  def generate(self, start_notes, start_durations, max_tokens , temperature):\n",
        "    attention_model = models.Model(\n",
        "        inputs = self.model.input,\n",
        "        outputs = self.model.get_layer(\"attention\").output,\n",
        "    )\n",
        "\n",
        "    start_note_tokens = [self.note_to_index.get(x, 1) for x in start_notes]\n",
        "    start_duration_tokens = [\n",
        "        self.duration_to_index.get(x, 1) for x in start_durations\n",
        "    ]\n",
        "    sample_note = None\n",
        "    sample_duration = None\n",
        "    info = []\n",
        "    midi_stream = music21.stream.Stream()\n",
        "\n",
        "    midi_strea.append(music21.clef.BassClef())\n",
        "\n",
        "    for sample_note, sample_duration in zip(start_notes, start_durations):\n",
        "      new_note = get_midi_note(sample_note, sample_duration)\n",
        "      if new_note is note None:\n",
        "        midi_stream.append(new_note)\n",
        "\n",
        "    while len(start_note_tokens) < max_tokens:\n",
        "      x1 = np.array([start_note_tokens])\n",
        "      x2 = np.array([start_duration_tokens])\n",
        "      notes , durations = self.model.predict([x1 , x2] , verbose = 0)\n",
        "\n",
        "      repeat = True\n",
        "\n",
        "      while repeat:\n",
        "        (\n",
        "            new_note,\n",
        "            sample_note_idx,\n",
        "            sample_note,\n",
        "            note_probs,\n",
        "            sample_duration_idx,\n",
        "            sample_duration,\n",
        "            duration_probs,\n",
        "        ) = self.get_note(notes, durations , temperature)\n",
        "\n",
        "        if (\n",
        "            isinstance(new_note, music21)\n",
        "        )"
      ],
      "metadata": {
        "id": "tDs1Fz4EPP_p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}