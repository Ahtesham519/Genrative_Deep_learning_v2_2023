{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP1pffCfALNDIcusoKZmrSo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ahtesham519/Genrative_Deep_learning_v2_2023/blob/main/Variational_AutoEncoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dSqx_Kivd7YI"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras import(\n",
        "    layers,\n",
        "    models,\n",
        "    datasets,\n",
        "    callbacks,\n",
        "    losses,\n",
        "    optimizers,\n",
        "    metrics,\n",
        ")\n",
        "\n",
        "from scipy.stats import norm\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#0. Parameters"
      ],
      "metadata": {
        "id": "49kCElGGeXiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = 32\n",
        "BATCH_SIZE = 100\n",
        "VALIDATION_SPLIT = 0.2\n",
        "EMBEDDING_DIM  = 2\n",
        "EPOCHS = 5\n",
        "BETA = 500"
      ],
      "metadata": {
        "id": "GU5dVPBQeWIe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Prepare the data"
      ],
      "metadata": {
        "id": "X5eOSaHveiXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the data\n",
        "(x_train, y_train) , (x_test, y_test) = datasets.fashion_mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rX99N4lzehXh",
        "outputId": "29783005-d489-4771-d3ea-70442a27dfdb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocess the data\n",
        "\n",
        "def preprocess(imgs):\n",
        "  \"\"\"\n",
        "  Normalize and reshape the images\n",
        "  \"\"\"\n",
        "  imgs = imgs.astype(\"float32\") / 255.0\n",
        "  imgs = np.pad(imgs , ((0,0) , (2,2) , (2,2)), constant_values = 0.0)\n",
        "  imgs = np.expand_dims(imgs, -1)\n",
        "  return imgs\n",
        "\n",
        "x_train = preprocess(x_train)\n",
        "x_test = preprocess(x_test)"
      ],
      "metadata": {
        "id": "t77kMKcNer7X"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Show some items of clothing from the training set\n",
        "display(x_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WFAAIqd7fDaU",
        "outputId": "16812315-54da-40fb-e028-9721d763dcc5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[[[0.],\n",
              "         [0.],\n",
              "         [0.],\n",
              "         ...,\n",
              "         [0.],\n",
              "         [0.],\n",
              "         [0.]],\n",
              "\n",
              "        [[0.],\n",
              "         [0.],\n",
              "         [0.],\n",
              "         ...,\n",
              "         [0.],\n",
              "         [0.],\n",
              "         [0.]],\n",
              "\n",
              "        [[0.],\n",
              "         [0.],\n",
              "         [0.],\n",
              "         ...,\n",
              "         [0.],\n",
              "         [0.],\n",
              "         [0.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0.],\n",
              "         [0.],\n",
              "         [0.],\n",
              "         ...,\n",
              "         [0.],\n",
              "         [0.],\n",
              "         [0.]],\n",
              "\n",
              "        [[0.],\n",
              "         [0.],\n",
              "         [0.],\n",
              "         ...,\n",
              "         [0.],\n",
              "         [0.],\n",
              "         [0.]],\n",
              "\n",
              "        [[0.],\n",
              "         [0.],\n",
              "         [0.],\n",
              "         ...,\n",
              "         [0.],\n",
              "         [0.],\n",
              "         [0.]]],\n",
              "\n",
              "\n",
              "       [[[0.],\n",
              "         [0.],\n",
              "         [0.],\n",
              "         ...,\n",
              "         [0.],\n",
              "         [0.],\n",
              "         [0.]],\n",
              "\n",
              "        [[0.],\n",
              "         [0.],\n",
              "         [0.],\n",
              "         ...,\n",
              "         [0.],\n",
              "         [0.],\n",
              "         [0.]],\n",
              "\n",
              "        [[0.],\n",
              "         [0.],\n",
              "         [0.],\n",
              "         ...,\n",
              "         [0.],\n",
              "         [0.],\n",
              "         [0.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0.],\n",
              "         [0.],\n",
              "         [0.],\n",
              "         ...,\n",
              "         [0.],\n",
              "         [0.],\n",
              "         [0.]],\n",
              "\n",
              "        [[0.],\n",
              "         [0.],\n",
              "         [0.],\n",
              "         ...,\n",
              "         [0.],\n",
              "         [0.],\n",
              "         [0.]],\n",
              "\n",
              "        [[0.],\n",
              "         [0.],\n",
              "         [0.],\n",
              "         ...,\n",
              "         [0.],\n",
              "         [0.],\n",
              "         [0.]]],\n",
              "\n",
              "\n",
              "       [[[0.],\n",
              "         [0.],\n",
              "         [0.],\n",
              "         ...,\n",
              "         [0.],\n",
              "         [0.],\n",
              "         [0.]],\n",
              "\n",
              "        [[0.],\n",
              "         [0.],\n",
              "         [0.],\n",
              "         ...,\n",
              "         [0.],\n",
              "         [0.],\n",
              "         [0.]],\n",
              "\n",
              "        [[0.],\n",
              "         [0.],\n",
              "         [0.],\n",
              "         ...,\n",
              "         [0.],\n",
              "         [0.],\n",
              "         [0.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0.],\n",
              "         [0.],\n",
              "         [0.],\n",
              "         ...,\n",
              "         [0.],\n",
              "         [0.],\n",
              "         [0.]],\n",
              "\n",
              "        [[0.],\n",
              "         [0.],\n",
              "         [0.],\n",
              "         ...,\n",
              "         [0.],\n",
              "         [0.],\n",
              "         [0.]],\n",
              "\n",
              "        [[0.],\n",
              "         [0.],\n",
              "         [0.],\n",
              "         ...,\n",
              "         [0.],\n",
              "         [0.],\n",
              "         [0.]]],\n",
              "\n",
              "\n",
              "       ...,\n",
              "\n",
              "\n",
              "       [[[0.],\n",
              "         [0.],\n",
              "         [0.],\n",
              "         ...,\n",
              "         [0.],\n",
              "         [0.],\n",
              "         [0.]],\n",
              "\n",
              "        [[0.],\n",
              "         [0.],\n",
              "         [0.],\n",
              "         ...,\n",
              "         [0.],\n",
              "         [0.],\n",
              "         [0.]],\n",
              "\n",
              "        [[0.],\n",
              "         [0.],\n",
              "         [0.],\n",
              "         ...,\n",
              "         [0.],\n",
              "         [0.],\n",
              "         [0.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0.],\n",
              "         [0.],\n",
              "         [0.],\n",
              "         ...,\n",
              "         [0.],\n",
              "         [0.],\n",
              "         [0.]],\n",
              "\n",
              "        [[0.],\n",
              "         [0.],\n",
              "         [0.],\n",
              "         ...,\n",
              "         [0.],\n",
              "         [0.],\n",
              "         [0.]],\n",
              "\n",
              "        [[0.],\n",
              "         [0.],\n",
              "         [0.],\n",
              "         ...,\n",
              "         [0.],\n",
              "         [0.],\n",
              "         [0.]]],\n",
              "\n",
              "\n",
              "       [[[0.],\n",
              "         [0.],\n",
              "         [0.],\n",
              "         ...,\n",
              "         [0.],\n",
              "         [0.],\n",
              "         [0.]],\n",
              "\n",
              "        [[0.],\n",
              "         [0.],\n",
              "         [0.],\n",
              "         ...,\n",
              "         [0.],\n",
              "         [0.],\n",
              "         [0.]],\n",
              "\n",
              "        [[0.],\n",
              "         [0.],\n",
              "         [0.],\n",
              "         ...,\n",
              "         [0.],\n",
              "         [0.],\n",
              "         [0.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0.],\n",
              "         [0.],\n",
              "         [0.],\n",
              "         ...,\n",
              "         [0.],\n",
              "         [0.],\n",
              "         [0.]],\n",
              "\n",
              "        [[0.],\n",
              "         [0.],\n",
              "         [0.],\n",
              "         ...,\n",
              "         [0.],\n",
              "         [0.],\n",
              "         [0.]],\n",
              "\n",
              "        [[0.],\n",
              "         [0.],\n",
              "         [0.],\n",
              "         ...,\n",
              "         [0.],\n",
              "         [0.],\n",
              "         [0.]]],\n",
              "\n",
              "\n",
              "       [[[0.],\n",
              "         [0.],\n",
              "         [0.],\n",
              "         ...,\n",
              "         [0.],\n",
              "         [0.],\n",
              "         [0.]],\n",
              "\n",
              "        [[0.],\n",
              "         [0.],\n",
              "         [0.],\n",
              "         ...,\n",
              "         [0.],\n",
              "         [0.],\n",
              "         [0.]],\n",
              "\n",
              "        [[0.],\n",
              "         [0.],\n",
              "         [0.],\n",
              "         ...,\n",
              "         [0.],\n",
              "         [0.],\n",
              "         [0.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0.],\n",
              "         [0.],\n",
              "         [0.],\n",
              "         ...,\n",
              "         [0.],\n",
              "         [0.],\n",
              "         [0.]],\n",
              "\n",
              "        [[0.],\n",
              "         [0.],\n",
              "         [0.],\n",
              "         ...,\n",
              "         [0.],\n",
              "         [0.],\n",
              "         [0.]],\n",
              "\n",
              "        [[0.],\n",
              "         [0.],\n",
              "         [0.],\n",
              "         ...,\n",
              "         [0.],\n",
              "         [0.],\n",
              "         [0.]]]], dtype=float32)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Build the variational autoencoder"
      ],
      "metadata": {
        "id": "jmuxslk0fJo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sampling(layers.Layer):\n",
        "  def call(self, inputs):\n",
        "    z_mean , z_log_var = inputs\n",
        "    batch = tf.shape(z_mean)[0]\n",
        "    dim = tf.shape(z_mean)[1]\n",
        "    epsilon = K.random_normal(shape = (batch, dim))\n",
        "    return z_mean +tf.exp(0.5 * z_log_var) * epsilon"
      ],
      "metadata": {
        "id": "0EDQWzaOfIhI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Encoder\n",
        "encoder_input = layers.input(\n",
        "    shape = (IMAGE_SIZE , IMAGE_SIZE, 1), name = \"encoder_input\"\n",
        ")\n",
        "x = layers.Conv2D(32, (3,3) , strides = 2, activation = \"relu\" , padding = \"same\")(\n",
        "    encoder_input\n",
        ")\n",
        "x = layers.Conv2D(64, (3,3) , strides = 2, activation =  \"relu\" , padding = \"same\")(x)\n",
        "x = layers.Conv2D(128, (3,3), strides = 2, activation = \"relu\" , padding = \"same\")(x)\n",
        "shape_before_flattening = K.int_shape(x)[1:] # the decoder will need this!\n",
        "\n",
        "x = layers.Flatten()(x)\n",
        "z_mean = layers.Dense(EMBEDDING_DIM, name = \"z_mean\")(x)\n",
        "z_log_var = layers.Dense(EMBEDDING_DIM , name = \"z_log_ver\")(x)\n",
        "z = Sampling()([z_mean , z_log_var])\n",
        "\n",
        "encoder = models.Model(encoder_input , [z_mean, z_log_var, z] , name=  \"encoder\")\n",
        "encoder.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "pgf4rpTafezb",
        "outputId": "b94e1afb-1314-49d5-c006-f6ed6ae7cb19"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'keras.api._v2.keras.layers' has no attribute 'input'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-fa5e4dece992>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m encoder_input = layers.input(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mIMAGE_SIZE\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mIMAGE_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"encoder_input\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m x = layers.Conv2D(32, (3,3) , strides = 2, activation = \"relu\" , padding = \"same\")(\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'keras.api._v2.keras.layers' has no attribute 'input'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Decoder\n",
        "decoder_input = layers.Input(shape = (EMBEDDING_DIM ,), name = \"decoder_input\")\n",
        "x = layers.Dense(np.prod(shape_before_flattening))(decoder_input)\n",
        "x = layers.Reshape(shape_before_flattening)(x)\n",
        "x = layers.Conv2DTranspose(\n",
        "    128, (3,3) , strides = 2, activation = \"relu\", padding = \"same\"\n",
        ")(x)\n",
        "x = layers.Conv2DTranspose(\n",
        "    32, (3,3) , strides = 2, activation = \"relu\", padding = \"same\"\n",
        ")(x)\n",
        "decoder_output = layers.Conv2D(\n",
        "    1,\n",
        "    (3,3),\n",
        "    strides = 1,\n",
        "    activation = \"sigmoid\",\n",
        "    padding = \"same\",\n",
        "    name = \"decoder_output\",\n",
        ")(x)\n",
        "\n",
        "decoder = models.Model(decoder_input , decoder_output)\n",
        "decoder_summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "prIPrHR1gcoQ",
        "outputId": "c0a0b3e8-7a2f-49d4-d7d0-214206beb23b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'shape_before_flattening' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-04e392a3e0fc>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdecoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mEMBEDDING_DIM\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"decoder_input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape_before_flattening\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape_before_flattening\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m x = layers.Conv2DTranspose(\n",
            "\u001b[0;31mNameError\u001b[0m: name 'shape_before_flattening' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(models.Model):\n",
        "  def __init__(self, encoder, decoder, **kwargs):\n",
        "    super(VAE, self).__init__(**kwargs)\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.total_loss_tracker = metrics.Mean(name = \"total_loss\")\n",
        "    self.reconstruction_loss_tracker = metrics.Mean(\n",
        "        name = \"reconstruction_loss\"\n",
        "    )\n",
        "    self.kl_loss_tracker = metrics.Mean(name = \"kl_loss\")\n",
        "\n",
        "  @property\n",
        "  def metrics(self):\n",
        "    return[\n",
        "        self.total_loss_tracker,\n",
        "        self.reconstruction_loss_tracker,\n",
        "        self.kl_loss_tracker,\n",
        "    ]\n",
        "\n",
        "  def call(self, inputs):\n",
        "    \"\"\"Call the model on a particular input\"\"\"\n",
        "    z_mean , z_log_var , z = encoder(inputs)\n",
        "    reconstruction = decoder(z)\n",
        "    return z_mean , z_log_var, reconstruction\n",
        "\n",
        "\n",
        "  def train_step(self, data):\n",
        "    \"\"\"Step run during training.\"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "      z_mean, z_log_var , reconstruction = self(data)\n",
        "      reconstruction_loss = tf.reduce_mean(\n",
        "          BETA\n",
        "          * losses.binary_crossentropy(\n",
        "              data, reconstruction, axis = (1,2,3)\n",
        "          )\n",
        "      )\n",
        "      kl_loss = tf.reduce_mean(\n",
        "          tf.reduce_sum(\n",
        "              -0.5\n",
        "              * (1 + z_log_var - tf.square(z_mean)  - tf.exp(z_log_var))\n",
        "              axis = 1,\n",
        "          )\n",
        "      )\n",
        "      total_loss = reconstruction_loss  + kl_loss\n",
        "\n",
        "    grads = tape.gradient(total_loss , self.trainable_weights)\n",
        "    self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "\n",
        "    self.total_loss_tracker.update_state(total_loss)\n",
        "    self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "    self.kl_loss_tracker.update_state(kl_loss)\n",
        "\n",
        "    return {m.name : m.result() for m in self.metrics}\n",
        "\n",
        "  def test_step(self, data):\n",
        "    \"\"\"Step run during validation.\"\"\"\n",
        "    if isinstance(data, tuple):\n",
        "      data = data[0]\n",
        "\n",
        "    z_mean , z_log_var , reconstruction = self(data)\n",
        "    reconstruction_loss = tf.reduce_mean(\n",
        "        BETA\n",
        "        * losses.binary_crossentropy(data, reconstruction, axis = (1, 2, 3))\n",
        "    )\n",
        "    kl_loss = tf.reduce_mean(\n",
        "        tf.reduce_sum(\n",
        "            -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)),\n",
        "            axis = 1,\n",
        "        )\n",
        "    )\n",
        "    total_loss = reconstruction_loss + kl_loss\n",
        "\n",
        "    return {\n",
        "        \"loss\":total_loss,\n",
        "        \"reconstruction_loss\":reconstruction_loss,\n",
        "        \"kl_loss\":kl_loss,\n",
        "    }\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "DPebt0yAhLXl",
        "outputId": "15545907-a6b1-4ab5-f0c4-1b4f9f39a94d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax. Perhaps you forgot a comma? (<ipython-input-9-f5721a3c254d>, line 39)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-f5721a3c254d>\"\u001b[0;36m, line \u001b[0;32m39\u001b[0m\n\u001b[0;31m    -0.5\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a variational autoencoder\n",
        "vae = VAE(encoder, decoeder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "35sXIgjcj3Wn",
        "outputId": "c97f1bb3-0ded-476a-cceb-ffbe015fd8bd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'VAE' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-fe7dfddcd9c9>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Create a variational autoencoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoeder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'VAE' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Train the variational autoencoder"
      ],
      "metadata": {
        "id": "YxyOmi5Pj8at"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#compile the variational autoencoder\n",
        "optimizer = optimizers.Adam(leanring_rate = 0.0005)\n",
        "vae.compile(optimizer = optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "PJMOLfIjj7l4",
        "outputId": "d1fe9ba0-20f4-4c5a-c6c9-9c0c225affef"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "leanring_rate is not a valid argument, kwargs should be empty  for `optimizer_experimental.Optimizer`.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-a5b72eded375>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#compile the variational autoencoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleanring_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/adam.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, learning_rate, beta_1, beta_2, epsilon, amsgrad, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, jit_compile, name, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     ):\n\u001b[0;32m--> 110\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    111\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, jit_compile, **kwargs)\u001b[0m\n\u001b[1;32m   1082\u001b[0m         \u001b[0mmesh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mesh\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mesh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmesh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1084\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m   1085\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m             \u001b[0mweight_decay\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, jit_compile, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iteration_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_create_iteration_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\u001b[0m in \u001b[0;36m_process_kwargs\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m                 )\n\u001b[1;32m    141\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m                 raise TypeError(\n\u001b[0m\u001b[1;32m    143\u001b[0m                     \u001b[0;34mf\"{k} is not a valid argument, kwargs should be empty \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                     \u001b[0;34m\" for `optimizer_experimental.Optimizer`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: leanring_rate is not a valid argument, kwargs should be empty  for `optimizer_experimental.Optimizer`."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a model save checkpoint\n",
        "model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
        "    filepath = \"./checkpoint\",\n",
        "    save_weights_only = False,\n",
        "    save_freq = \"epoch\",\n",
        "    monitor = \"loss\",\n",
        "    mode = \"min\",\n",
        "    save_best_only = True,\n",
        "    versbose = 0,\n",
        ")\n",
        "tensorboard_callback = callbacks.TensorBoard(lod_dir = \"./logs\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "W1ddoHs9kMnt",
        "outputId": "5d175da8-9a17-4c44-839a-8c0fae1b31c2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Unrecognized arguments in `TensorBoard` Callback: {'lod_dir'}. Supported kwargs are: {'embeddings_data', 'batch_size', 'write_grads', 'embeddings_layer_names'}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-65ef1f0a24f1>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mversbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtensorboard_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorBoard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlod_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./logs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/callbacks.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, log_dir, histogram_freq, write_graph, write_images, write_steps_per_second, update_freq, profile_batch, embeddings_freq, embeddings_metadata, **kwargs)\u001b[0m\n\u001b[1;32m   2500\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2501\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_supports_tf_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2502\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2504\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/callbacks.py\u001b[0m in \u001b[0;36m_validate_kwargs\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m   2559\u001b[0m         \u001b[0;31m# Only allow kwargs that were supported in V1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2560\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0munrecognized_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2561\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   2562\u001b[0m                 \u001b[0;34m\"Unrecognized arguments in `TensorBoard` Callback: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2563\u001b[0m                 \u001b[0;34mf\"{unrecognized_kwargs}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unrecognized arguments in `TensorBoard` Callback: {'lod_dir'}. Supported kwargs are: {'embeddings_data', 'batch_size', 'write_grads', 'embeddings_layer_names'}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vae.fit(\n",
        "    x_train,\n",
        "    epochs  = EPOCHS,\n",
        "    batch_size = BATCH_SIZE ,\n",
        "    shuffle = True,\n",
        "    validation_data = (x_test , x_test),\n",
        "    callbacks = [model_checkpoint_callback, tensorboard_callback],\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "K9k2x910klS6",
        "outputId": "27d92e9e-d5d5-451e-b931-b629a05f7df8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'vae' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-b97d5720d0b9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m vae.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mepochs\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vae' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save the final models\n",
        "vae.save(\"./models/vae\")\n",
        "encoder.save(\"./models/encoer\")\n",
        "decoder.save(\"./models/decoder\")"
      ],
      "metadata": {
        "id": "GgGrGJGTk0R7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Reconstruct using the variational autoencoder"
      ],
      "metadata": {
        "id": "XFF1LOadk-Us"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Select a subset of the test set\n",
        "n_to_predict = 5000\n",
        "example_images = x_test[:n_to_predict]\n",
        "example_labels = y_test[:n_to_predict]\n"
      ],
      "metadata": {
        "id": "F_AggNc8k9hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create autoencoder predictions and display\n",
        "z_mean, z_log_var , reconstruction = vae.predict(example_images)\n",
        "print(\"Example real clothing items\")\n",
        "display(example_images)\n",
        "print(\"reconstructions\")\n",
        "display(reconstructions)\n"
      ],
      "metadata": {
        "id": "wcBO4ILUlNkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Embed using the encoder"
      ],
      "metadata": {
        "id": "PcfH0dylmLQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Encode the exapmle images\n",
        "z_mean, z_var , z = encoder(example_images)"
      ],
      "metadata": {
        "id": "RZSDKxRolpK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#some exapmle of the embeddings\n",
        "print(z[:10])"
      ],
      "metadata": {
        "id": "SQQ2BtgBmTEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Show the encoded points in 2D space\n",
        "figsize = 8\n",
        "\n",
        "plt.figure(figsize = (figsize, figsize))\n",
        "plt.scatter(z[:, 0]  z[:, 1] , c = \"black\" , alpha = 0.5 , s= 3)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cpQIBQz8mXKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Generate using the decoder"
      ],
      "metadata": {
        "id": "Vca3m_5bmtJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sample some points in the latent space , from the standard normal distribution\n",
        "grid_with , grid_height = (6, 3)\n",
        "z_sample = np.random.normal(size = (grid_width * grid_height, 2))"
      ],
      "metadata": {
        "id": "5GfN3Q2NmsdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Decode the smapled points\n",
        "reconstructions = decoder.predict(z_sample)\n"
      ],
      "metadata": {
        "id": "H-TgqTX9m_OD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert orignal embeddings and sampled embeddings to p-value\n",
        "p = norm.cdf(z)\n",
        "p_sample = norm.cdf(z_sample)"
      ],
      "metadata": {
        "id": "dqKZrfQnnE-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Draw a plot of\n",
        "figsize = 8\n",
        "plt.figure(figsize = (figsize , figsize))\n",
        "\n",
        "# ... the orignal embeddings ...\n",
        "plt.scatter(z[:, 0] , z[: ,1 ], c = \"black\" , alpha = 0.5, s = 2)\n",
        "\n",
        "# ... and the newly generated points in the latent space\n",
        "plt.scatter(z_sample[:, 0] , z_sample[:,1] , c = \"#00B0F0\" , alpha = 1, s = 40)\n",
        "plt.show()\n",
        "\n",
        "#Add underneath a grid of the decoded images\n",
        "fig = plt.figure(figsize = (figsize, grid_height * 2))\n",
        "fig.subplots_adjust(hspace = 0.4 , wspace = 0.4)\n",
        "\n",
        "for i in range(grid_width * grid_height):\n",
        "  ax = fig.add_subplot(grid_height , grid_width , 1 + i)\n",
        "  ax.axis(\"off\")\n",
        "  ax.text(\n",
        "      0.5,\n",
        "      -0.35,\n",
        "      str(np.round(z_sample[i, :] , 1)),\n",
        "      fontsize = 10,\n",
        "      ha = \"center\",\n",
        "      transform = ax.transAxes,\n",
        "  )\n",
        "  ax.imshow(reconstructions[i, :, :] , cmap = \"Greys\")"
      ],
      "metadata": {
        "id": "YTd5U1kQnNsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Explore the latent space"
      ],
      "metadata": {
        "id": "ei4xMRRkoUm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Color the embeddings by their label (clothing type - see table)\n",
        "figsize = 8\n",
        "fig = plt.figure(figsize = (figsize * 2, figsize))\n",
        "ax = fig.add_subplot(1, 2,1)\n",
        "plot_1 = ax.scatter(\n",
        "    z[:, 0] , z[:, 1] , cmap = \"rainbow\", c = example_labels , alpha = 0.8, s = 3\n",
        ")\n",
        "plt.colorbar(plot_1)\n",
        "ax = fig.add_subplot(1,2,2)\n",
        "plot_2 = ax.scatter(\n",
        "    p[:, 0] , p[: ,1 ] , cmap = \"rainbow\" , c = example_labels , alpha = 0.8 , s = 3\n",
        ")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rjik3ZpOoTk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#color the embeddings by their label (clothing type - )\n",
        "figsize = 12\n",
        "grid_size = 15\n",
        "plt.figure(figsize = (figsize , figsize))\n",
        "plt.scatter(\n",
        "    p[:, 0],p[:,1], cmap = \"rainbow\" , c = example_labels, alpha = 0.8, s = 300\n",
        ")\n",
        "plt.colorbar()\n",
        "\n",
        "x = norm.ppf(np.linspace(0, 1 , grid_size))\n",
        "y = norm.ppf(np.linspace(1,0 , grid_size))\n",
        "xv, yv = np.meshgrid(x, y)\n",
        "xv = xv.flatten()\n",
        "yv = yv.flatten()\n",
        "grid = np.arry(list(zip(xv, yv)))\n",
        "\n",
        "reconstructions = decoder.predict(grid)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "fig = plt.figure(figsize= (figsize, figsize))\n",
        "fig.subplots_adjust(hspace = 0.4, wspace = 0.4)\n",
        "for i in range(grid_size ** 2):\n",
        "  ax = fig.add_subplot(grid_size , grid_size, i + 1)\n",
        "  ax.axis(\"off\")\n",
        "  ax.imshow(reconstructions[i, :, :], cmap = \"Greys\")"
      ],
      "metadata": {
        "id": "xz1PQ6OAo9EQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oQ-pIncLqCJ5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}